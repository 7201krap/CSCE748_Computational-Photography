{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch_pconv import PConv2d  # Using the torch_pconv implementation\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pair_color_depth_files(root_dir):\n",
        "    color_dict = {}\n",
        "    depth_dict = {}\n",
        "    \n",
        "    for directory, _, files in os.walk(root_dir):\n",
        "        for f in files:\n",
        "            full_path = os.path.join(directory, f)\n",
        "            \n",
        "            # Determine a prefix based on whether we're in a subfolder or not.\n",
        "            if directory == root_dir:\n",
        "                prefix_prefix = \"\"  # Flat structure: don't add folder name.\n",
        "            else:\n",
        "                prefix_prefix = os.path.basename(directory) + \"_\"\n",
        "            \n",
        "            # Check file naming conventions and strip specific suffixes:\n",
        "            if f.endswith(\"_colors.png\"):\n",
        "                prefix = prefix_prefix + f.replace(\"_colors.png\", \"\")\n",
        "                color_dict[prefix] = full_path\n",
        "            elif f.endswith(\"_depth.png\"):\n",
        "                prefix = prefix_prefix + f.replace(\"_depth.png\", \"\")\n",
        "                depth_dict[prefix] = full_path\n",
        "            elif f.endswith(\".jpg\"):\n",
        "                # For jpg files assume they are color images.\n",
        "                prefix = prefix_prefix + f.replace(\".jpg\", \"\")\n",
        "                color_dict[prefix] = full_path\n",
        "            elif f.endswith(\".png\"):\n",
        "                # For png files not already handled, assume they are depth images.\n",
        "                prefix = prefix_prefix + f.replace(\".png\", \"\")\n",
        "                depth_dict[prefix] = full_path\n",
        "    \n",
        "    pairs = []\n",
        "    for prefix, cpath in color_dict.items():\n",
        "        if prefix in depth_dict:\n",
        "            pairs.append((cpath, depth_dict[prefix]))\n",
        "    return pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NYUDepthDataset(Dataset):\n",
        "    def __init__(self, root_dir, img_size=(480, 640), transform=None, apply_mask=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Path to the folder containing color/depth pairs (e.g. nyu2_train or nyu2_test).\n",
        "            img_size: Desired (height, width) for resizing.\n",
        "            transform: Optional transforms (e.g., Torchvision transforms) after resizing/normalizing.\n",
        "            apply_mask: Whether to create and apply a random mask.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "        self.apply_mask = apply_mask\n",
        "\n",
        "        # Create a list of (color_path, depth_path) pairs\n",
        "        self.samples = pair_color_depth_files(root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        color_path, depth_path = self.samples[idx]\n",
        "\n",
        "        # print(\"color_depth:\", color_path)\n",
        "        # print(\"depth_path:\",  depth_path)\n",
        "\n",
        "        # ----- Load color image ----- \n",
        "        img = cv2.imread(color_path)\n",
        "        # Convert BGR to RGB\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # Resize to (width, height) for OpenCV\n",
        "        img = cv2.resize(img, (self.img_size[1], self.img_size[0]))\n",
        "        # Normalize to [0, 1]\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "\n",
        "        # ----- Load depth image -----\n",
        "        depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
        "        depth = cv2.resize(depth, (self.img_size[1], self.img_size[0]))\n",
        "        depth = depth.astype(np.float32) / 255.0  # Adjust as needed\n",
        "\n",
        "        # If apply_mask is True, create a random mask and apply it\n",
        "        if self.apply_mask:\n",
        "            mask_2d = self.create_random_mask(self.img_size)\n",
        "            # Expand mask to 3 channels for the color image\n",
        "            mask_3d = np.stack([mask_2d]*3, axis=-1)\n",
        "\n",
        "            # Make a copy for the masked versions\n",
        "            masked_img = img.copy()\n",
        "            masked_depth = depth.copy()\n",
        "\n",
        "            # Where mask == 0, set color to 1.0 (white) and depth to 0\n",
        "            masked_img[mask_3d == 0] = 1.0\n",
        "            masked_depth[mask_2d == 0] = 0.0\n",
        "        else:\n",
        "            # If no mask, just pass the original images\n",
        "            mask_2d = np.ones((self.img_size[0], self.img_size[1]), dtype=np.float32) * 255\n",
        "            masked_img = img\n",
        "            masked_depth = depth\n",
        "\n",
        "        # ----- Convert to Torch Tensors -----\n",
        "        # Color images: (H, W, C) -> (C, H, W)\n",
        "        img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "        masked_img_tensor = torch.from_numpy(masked_img).permute(2, 0, 1)\n",
        "\n",
        "        # Depth images: (H, W) -> (1, H, W)\n",
        "        depth_tensor = torch.from_numpy(depth).unsqueeze(0)\n",
        "        masked_depth_tensor = torch.from_numpy(masked_depth).unsqueeze(0)\n",
        "\n",
        "        # Mask: (H, W) -> (1, H, W), normalized to [0, 1]\n",
        "        mask_tensor = torch.from_numpy(mask_2d.astype(np.float32) / 255.0).unsqueeze(0)\n",
        "\n",
        "        # Optionally apply user-supplied transforms (e.g. augmentations)\n",
        "        if self.transform is not None:\n",
        "            # Typically you might do: (But be careful with how transforms are applied to multi-channel data)\n",
        "            img_tensor = self.transform(img_tensor)\n",
        "            masked_img_tensor = self.transform(masked_img_tensor)\n",
        "            # For depth or mask, you might have a separate transform pipeline or skip\n",
        "\n",
        "        return {\n",
        "            \"image\": img_tensor,          # Original color image\n",
        "            \"masked_image\": masked_img_tensor,\n",
        "            \"depth\": depth_tensor,        # Original depth map\n",
        "            \"masked_depth\": masked_depth_tensor,\n",
        "            \"mask\": mask_tensor\n",
        "        }\n",
        "\n",
        "    def create_random_mask(self, size):\n",
        "        \"\"\"\n",
        "        Create a random mask of shape (H, W) using random lines.\n",
        "        White=255 => unmasked, Black=0 => masked.\n",
        "        \"\"\"\n",
        "        H, W = size\n",
        "        mask = np.full((H, W), 255, np.uint8)\n",
        "\n",
        "        # Draw a random number of lines\n",
        "        num_lines = np.random.randint(1, 10)\n",
        "        for _ in range(num_lines):\n",
        "            x1, x2 = np.random.randint(0, W, size=2)\n",
        "            y1, y2 = np.random.randint(0, H, size=2)\n",
        "            thickness = np.random.randint(1, 3)\n",
        "            cv2.line(mask, (x1, y1), (x2, y2), 0, thickness)\n",
        "\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Example: Using the train folder\n",
        "train_root = \"nyu_data/data/nyu2_train\"\n",
        "train_dataset = NYUDepthDataset(root_dir=train_root, img_size=(240, 320), apply_mask=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Example: Using the test folder\n",
        "test_root = \"nyu_data/data/nyu2_test\"\n",
        "test_dataset = NYUDepthDataset(root_dir=test_root, img_size=(240, 320), apply_mask=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(\"total number of train dataset:\", len(train_dataset))\n",
        "print(\"total number of test dataset:\",  len(test_dataset))\n",
        "\n",
        "# Get one batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "# print(\"Keys in the batch:\", sample_batch.keys())\n",
        "# print(\"image shape:\", sample_batch[\"image\"].shape)               # torch.Size([4, 3, 480, 640])\n",
        "# print(\"masked_image shape:\", sample_batch[\"masked_image\"].shape) # torch.Size([4, 3, 480, 640])\n",
        "# print(\"depth shape:\", sample_batch[\"depth\"].shape)               # torch.Size([4, 1, 480, 640])\n",
        "# print(\"masked_depth shape:\", sample_batch[\"masked_depth\"].shape) # torch.Size([4, 1, 480, 640])\n",
        "# print(\"mask shape:\", sample_batch[\"mask\"].shape)                 # torch.Size([4, 1, 480, 640])\n",
        "\n",
        "# sample_batch = next(iter(test_loader))\n",
        "# print(\"Keys in the batch:\", sample_batch.keys())\n",
        "# print(\"image shape:\", sample_batch[\"image\"].shape)\n",
        "# print(\"masked_image shape:\", sample_batch[\"masked_image\"].shape)\n",
        "# print(\"depth shape:\", sample_batch[\"depth\"].shape)\n",
        "# print(\"masked_depth shape:\", sample_batch[\"masked_depth\"].shape)\n",
        "# print(\"mask shape:\", sample_batch[\"mask\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_sample(sample_dict, idx=0):\n",
        "    \"\"\"\n",
        "    sample_dict is the dictionary returned by the DataLoader:\n",
        "      {\n",
        "        \"image\": (B, 3, H, W),\n",
        "        \"masked_image\": (B, 3, H, W),\n",
        "        \"depth\": (B, 1, H, W),\n",
        "        \"masked_depth\": (B, 1, H, W),\n",
        "        \"mask\": (B, 1, H, W)\n",
        "      }\n",
        "    idx is which item in the batch to visualize.\n",
        "    \"\"\"\n",
        "    # Convert Torch tensors -> NumPy, shape (H, W, C). Tensor, shape (C, H, W)\n",
        "    img_np = sample_dict[\"image\"][idx].permute(1, 2, 0).cpu().numpy()\n",
        "    masked_img_np = sample_dict[\"masked_image\"][idx].permute(1, 2, 0).cpu().numpy()\n",
        "    depth_np = sample_dict[\"depth\"][idx].squeeze(0).cpu().numpy()\n",
        "    masked_depth_np = sample_dict[\"masked_depth\"][idx].squeeze(0).cpu().numpy()\n",
        "    mask_np = sample_dict[\"mask\"][idx].squeeze(0).cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "    axs[0, 0].imshow(img_np)\n",
        "    axs[0, 0].set_title(\"Original Image\")\n",
        "\n",
        "    axs[0, 1].imshow(mask_np, cmap='gray')\n",
        "    axs[0, 1].set_title(\"Random Mask\")\n",
        "\n",
        "    axs[0, 2].imshow(masked_img_np)\n",
        "    axs[0, 2].set_title(\"Masked Image\")\n",
        "\n",
        "    axs[1, 0].imshow(depth_np, cmap='gray')\n",
        "    axs[1, 0].set_title(\"Original Depth\")\n",
        "\n",
        "    axs[1, 1].imshow(masked_depth_np, cmap='gray')\n",
        "    axs[1, 1].set_title(\"Masked Depth\")\n",
        "\n",
        "    axs[1, 2].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize first sample in the batch\n",
        "show_sample(sample_batch, idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Partial Convolution Based Autoencoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.pconv1 = PConv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
        "                              stride=1, padding=padding, bias=True)\n",
        "        self.pconv2 = PConv2d(out_channels, out_channels, kernel_size=kernel_size, \n",
        "                              stride=2, padding=padding, bias=True)\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        # Ensure mask is of shape (batch, H, W)\n",
        "        if mask.dim() == 4 and mask.size(1) == 1:\n",
        "            mask = mask.squeeze(1)\n",
        "        x1, mask1 = self.pconv1(x, mask)\n",
        "        x1 = F.relu(x1)\n",
        "        x2, mask2 = self.pconv2(x1, mask1)\n",
        "        x2 = F.relu(x2)\n",
        "        return x1, mask1, x2, mask2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels1, out_channels2, kernel_size=3, padding=1):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.pconv1 = PConv2d(skip_channels + in_channels, out_channels1, kernel_size=kernel_size, \n",
        "                              stride=1, padding=padding, bias=True)\n",
        "        self.pconv2 = PConv2d(out_channels1, out_channels2, kernel_size=kernel_size, \n",
        "                              stride=1, padding=padding, bias=True)\n",
        "    \n",
        "    def forward(self, x, mask, skip_x, skip_mask):\n",
        "        # Upsample feature map.\n",
        "        x_up = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        \n",
        "        # Ensure mask has proper shape before interpolation.\n",
        "        if mask.dim() == 4 and mask.size(1) == 1:\n",
        "            mask = mask.squeeze(1)\n",
        "        mask_up = F.interpolate(mask.unsqueeze(1), scale_factor=2, mode='nearest').squeeze(1)\n",
        "        \n",
        "        if skip_mask.dim() == 4 and skip_mask.size(1) == 1:\n",
        "            skip_mask = skip_mask.squeeze(1)\n",
        "        # Instead of concatenating masks, combine them via maximum.\n",
        "        mask_cat = torch.max(skip_mask, mask_up)\n",
        "        \n",
        "        # Concatenate image features along the channel dimension.\n",
        "        x_cat = torch.cat([skip_x, x_up], dim=1)\n",
        "        \n",
        "        x1, mask1 = self.pconv1(x_cat, mask_cat)\n",
        "        x1 = F.relu(x1)\n",
        "        x2, mask2 = self.pconv2(x1, mask1)\n",
        "        x2 = F.relu(x2)\n",
        "        return x1, mask1, x2, mask2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InpaintingModel(nn.Module):\n",
        "    def __init__(self, input_channels=3):\n",
        "        super(InpaintingModel, self).__init__()\n",
        "        self.enc1 = EncoderBlock(input_channels, 16)\n",
        "        self.enc2 = EncoderBlock(16, 32)\n",
        "        self.enc3 = EncoderBlock(32, 64)\n",
        "        self.enc4 = EncoderBlock(64, 128)\n",
        "        \n",
        "        self.dec1 = DecoderBlock(128, 128, 128, 64)\n",
        "        self.dec2 = DecoderBlock(64, 64, 64, 32)\n",
        "        self.dec3 = DecoderBlock(32, 32, 32, 16)\n",
        "        self.dec4 = DecoderBlock(16, 16, 16, 3)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(3, 3, kernel_size=3, padding=1)\n",
        "        \n",
        "    def forward(self, image, mask):\n",
        "        # Ensure mask shape is (batch, H, W) before any processing.\n",
        "        if mask.dim() == 4 and mask.size(1) == 1:\n",
        "            mask = mask.squeeze(1)\n",
        "        \n",
        "        conv1, mask1, conv2, mask2 = self.enc1(image, mask)\n",
        "        conv3, mask3, conv4, mask4 = self.enc2(conv2, mask2)\n",
        "        conv5, mask5, conv6, mask6 = self.enc3(conv4, mask4)\n",
        "        conv7, mask7, conv8, mask8 = self.enc4(conv6, mask6)\n",
        "        \n",
        "        conv9, mask9, conv10, mask10 = self.dec1(conv8, mask8, conv7, mask7)\n",
        "        conv11, mask11, conv12, mask12 = self.dec2(conv10, mask10, conv5, mask5)\n",
        "        conv13, mask13, conv14, mask14 = self.dec3(conv12, mask12, conv3, mask3)\n",
        "        conv15, mask15, conv16, mask16 = self.dec4(conv14, mask14, conv1, mask1)\n",
        "        \n",
        "        out = self.final_conv(conv16)\n",
        "        return torch.sigmoid(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dice_coef(y_true, y_pred, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Dice coefficient metric.\n",
        "    \"\"\"\n",
        "    y_true_f = y_true.view(-1)\n",
        "    y_pred_f = y_pred.view(-1)\n",
        "    intersection = (y_true_f * y_pred_f).sum()\n",
        "    return (2. * intersection + epsilon) / (y_true_f.sum() + y_pred_f.sum() + epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up device, model, optimizer, and loss function.\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)\n",
        "model = InpaintingModel(input_channels=3).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.L1Loss()  # Mean Absolute Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in tqdm(dataloader):\n",
        "        # Retrieve inputs from the batch.\n",
        "        masked_image = batch[\"masked_image\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        target = batch[\"image\"].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(masked_image, mask)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() * masked_image.size(0)\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total_dice = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            masked_image = batch[\"masked_image\"].to(device)\n",
        "            mask = batch[\"mask\"].to(device)\n",
        "            target = batch[\"image\"].to(device)\n",
        "            output = model(masked_image, mask)\n",
        "            loss = criterion(output, target)\n",
        "            running_loss += loss.item() * masked_image.size(0)\n",
        "            total_dice += dice_coef(target, output).item() * masked_image.size(0)\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_dice = total_dice / len(dataloader.dataset)\n",
        "    return epoch_loss, epoch_dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_dice = validate(model, test_loader, criterion, device)\n",
        "    \n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMbfu5iG1HOnVkbr6fZC+b9",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Image Inpainting Partial Convolution",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
