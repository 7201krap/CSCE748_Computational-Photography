{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab1a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_pconv import PConv2d  # Partial convolution layer; ensure this is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0468dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "# Encoder module that downsamples the input and returns skip connections.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, base_channels=8, levels=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.levels = levels\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(levels):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            self.blocks.append(conv_block(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "        return skips, x  # return list of skip features and bottleneck\n",
    "\n",
    "# Decoder block that upsamples and fuses with the skip connection.\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = conv_block(in_channels + skip_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Decoder that uses a list of DecoderBlocks.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, bottleneck_channels, skip_channels, levels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.levels = levels\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_channels = bottleneck_channels\n",
    "        # Skip channels are provided in order from shallowest to deepest.\n",
    "        for i in range(levels):\n",
    "            # Use skip from the reverse order.\n",
    "            skip_ch = skip_channels[-(i+1)]\n",
    "            out_channels = skip_ch // 2\n",
    "            self.blocks.append(DecoderBlock(in_channels, skip_ch, out_channels))\n",
    "            in_channels = out_channels\n",
    "        self.final_conv = nn.Conv2d(in_channels, 3, kernel_size=1)  # Output 3 channels\n",
    "\n",
    "    def forward(self, x, skips):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            skip = skips[-(i+1)]\n",
    "            x = block(x, skip)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "# Simple encoder-decoder network for inpainting.\n",
    "class SimpleInpaintingNet(nn.Module):\n",
    "    def __init__(self, base_channels=8, levels=3):\n",
    "        super(SimpleInpaintingNet, self).__init__()\n",
    "        # One encoder for the masked input image (3 channels).\n",
    "        self.encoder = Encoder(in_channels=3, base_channels=base_channels, levels=levels)\n",
    "        \n",
    "        # Since we are using only one encoder, the skip features are not doubled.\n",
    "        # Each encoder level produces channels: base_channels * (2**i)\n",
    "        skip_channels = [base_channels * (2 ** i) for i in range(levels)]\n",
    "        # The bottleneck has channels equal to the last encoder block's output:\n",
    "        bottleneck_channels = base_channels * (2 ** (levels - 1))\n",
    "        \n",
    "        self.decoder = Decoder(bottleneck_channels=bottleneck_channels, skip_channels=skip_channels, levels=levels)\n",
    "        \n",
    "    def forward(self, masked_img):\n",
    "        skips, bottleneck = self.encoder(masked_img)\n",
    "        out = self.decoder(bottleneck, skips)\n",
    "        return torch.sigmoid(out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85195cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple convolutional block using built-in Sequential.\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "# Encoder module that downsamples the input and returns skip connections.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, base_channels=8, levels=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.levels = levels\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(levels):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            self.blocks.append(conv_block(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "        return skips, x  # return list of skip features and bottleneck\n",
    "\n",
    "# Decoder block that upsamples and fuses with the skip connection.\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = conv_block(in_channels + skip_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Decoder that uses a list of DecoderBlocks.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, bottleneck_channels, skip_channels, levels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.levels = levels\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_channels = bottleneck_channels\n",
    "        # skip_channels should be provided in order from shallowest to deepest.\n",
    "        for i in range(levels):\n",
    "            # Use skip from the reverse order.\n",
    "            skip_ch = skip_channels[-(i+1)]\n",
    "            out_channels = skip_ch // 2\n",
    "            self.blocks.append(DecoderBlock(in_channels, skip_ch, out_channels))\n",
    "            in_channels = out_channels\n",
    "        self.final_conv = nn.Conv2d(in_channels, 3, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x, skips):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            skip = skips[-(i+1)]\n",
    "            x = block(x, skip)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SimpleAttentionFusion(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of channels from each branch (assumed equal).\n",
    "            reduction (int): Factor to reduce the number of channels for the attention computation.\n",
    "        \"\"\"\n",
    "        super(SimpleAttentionFusion, self).__init__()\n",
    "        # Here we concatenate RGB and depth features: total channels = 2 * in_channels\n",
    "        # The conv layers compute an attention map that will be applied to the RGB features.\n",
    "        self.conv1   = nn.Conv2d(in_channels * 2, in_channels // reduction, kernel_size=1)\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.conv2   = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, rgb_encoded, depth_encoded):\n",
    "        # Concatenate the RGB and depth features along the channel dimension.\n",
    "        combined = torch.cat([rgb_encoded, depth_encoded], dim=1)\n",
    "        # Compute a lower-dimensional representation.\n",
    "        attn = self.conv1(combined)\n",
    "        attn = self.relu(attn)\n",
    "        # Map back to the original channel dimension.\n",
    "        attn = self.conv2(attn)\n",
    "        # Convert to an attention mask with values in the range [0, 1].\n",
    "        attn = self.sigmoid(attn)\n",
    "        # Use the attention mask to modulate the RGB features, and fuse with depth via addition.\n",
    "        fused = rgb_encoded * attn + depth_encoded\n",
    "        return fused\n",
    "\n",
    "\n",
    "class DepthEnhancedInpaintingNet_SimpleAttention(nn.Module):\n",
    "    def __init__(self, base_channels=8, levels=3, reduction=4):\n",
    "        super(DepthEnhancedInpaintingNet_SimpleAttention, self).__init__()\n",
    "        # Encoders for RGB (3 channels) and depth (1 channel)\n",
    "        self.rgb_encoder   = Encoder(in_channels=3, base_channels=base_channels, levels=levels)\n",
    "        self.depth_encoder = Encoder(in_channels=1, base_channels=base_channels, levels=levels)\n",
    "        \n",
    "        # Bottleneck channels for each encoder (last block channels)\n",
    "        bottleneck_channels = base_channels * (2 ** (levels - 1))\n",
    "        \n",
    "        # Replace the multihead attention fusion with a simple attention fusion module.\n",
    "        self.fusion = SimpleAttentionFusion(in_channels=bottleneck_channels, reduction=reduction)\n",
    "        \n",
    "        # For the skip features, we can continue to fuse by concatenation if desired.\n",
    "        fused_skips = [base_channels * (2 ** i) * 2 for i in range(levels)]\n",
    "        self.decoder = Decoder(bottleneck_channels=bottleneck_channels, skip_channels=fused_skips, levels=levels)\n",
    "        \n",
    "    def forward(self, rgb, depth):\n",
    "        rgb_skips, rgb_bottleneck     = self.rgb_encoder(rgb)\n",
    "        depth_skips, depth_bottleneck = self.depth_encoder(depth)\n",
    "        \n",
    "        # Fuse the bottleneck features with the simple attention mechanism.\n",
    "        fused_bottleneck = self.fusion(rgb_bottleneck, depth_bottleneck)\n",
    "        \n",
    "        # Fuse skip connections by simple concatenation.\n",
    "        fused_skips = [torch.cat([r, d], dim=1) for r, d in zip(rgb_skips, depth_skips)]\n",
    "        \n",
    "        # Decode to produce the inpainted RGB image.\n",
    "        out = self.decoder(fused_bottleneck, fused_skips)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de622b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple convolutional block using built-in Sequential.\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "# Encoder module that downsamples the input and returns skip connections.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, base_channels=8, levels=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.levels = levels\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(levels):\n",
    "            out_channels = base_channels * (2 ** i)\n",
    "            self.blocks.append(conv_block(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "        return skips, x  # return list of skip features and bottleneck\n",
    "\n",
    "# Decoder block that upsamples and fuses with the skip connection.\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = conv_block(in_channels + skip_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Decoder that uses a list of DecoderBlocks.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, bottleneck_channels, skip_channels, levels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.levels = levels\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_channels = bottleneck_channels\n",
    "        # skip_channels should be provided in order from shallowest to deepest.\n",
    "        for i in range(levels):\n",
    "            # Use skip from the reverse order.\n",
    "            skip_ch = skip_channels[-(i+1)]\n",
    "            out_channels = skip_ch // 2\n",
    "            self.blocks.append(DecoderBlock(in_channels, skip_ch, out_channels))\n",
    "            in_channels = out_channels\n",
    "        self.final_conv = nn.Conv2d(in_channels, 3, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x, skips):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            skip = skips[-(i+1)]\n",
    "            x = block(x, skip)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# A self-attention module using PyTorch's built-in MultiheadAttention.\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim, num_heads=4):\n",
    "        super(MultiheadSelfAttention, self).__init__()\n",
    "        # nn.MultiheadAttention expects input shape (B, L, C) if batch_first=True.\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=in_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W) --> flatten spatial dims to sequence: (B, H*W, C)\n",
    "        B, C, H, W = x.size()\n",
    "        x_flat = x.view(B, C, H * W).permute(0, 2, 1)  # now (B, L, C) with L=H*W\n",
    "        attn_output, _ = self.mha(x_flat, x_flat, x_flat)\n",
    "        # reshape back to (B, C, H, W)\n",
    "        attn_output = attn_output.permute(0, 2, 1).view(B, C, H, W)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# Full model: Dual-branch encoder for RGB and depth, attention fusion, then decoder.\n",
    "class DepthEnhancedInpaintingNet(nn.Module):\n",
    "    def __init__(self, base_channels=8, levels=3, num_heads=4):\n",
    "        super(DepthEnhancedInpaintingNet, self).__init__()\n",
    "        # Encoders for RGB (3 channels) and depth (1 channel)\n",
    "        self.rgb_encoder   = Encoder(in_channels=3, base_channels=base_channels, levels=levels)\n",
    "        self.depth_encoder = Encoder(in_channels=1, base_channels=base_channels, levels=levels)\n",
    "        \n",
    "        # Bottleneck channels for each encoder (last block channels)\n",
    "        bottleneck_rgb   = base_channels * (2 ** (levels - 1))\n",
    "        bottleneck_depth = base_channels * (2 ** (levels - 1))\n",
    "        fused_bottleneck_channels = bottleneck_rgb + bottleneck_depth\n",
    "        \n",
    "        # Fuse bottleneck features with multihead self-attention\n",
    "        self.attention = MultiheadSelfAttention(in_dim=fused_bottleneck_channels, num_heads=num_heads)\n",
    "        \n",
    "        # Skip channels: each encoder level produces channels = base_channels * (2**i)\n",
    "        # Fused skip channels from both branches are doubled.\n",
    "        fused_skips = [base_channels * (2 ** i) * 2 for i in range(levels)]\n",
    "        self.decoder = Decoder(bottleneck_channels=fused_bottleneck_channels, skip_channels=fused_skips, levels=levels)\n",
    "        \n",
    "    def forward(self, rgb, depth):\n",
    "        rgb_skips, rgb_bottleneck     = self.rgb_encoder(rgb)\n",
    "        depth_skips, depth_bottleneck = self.depth_encoder(depth)\n",
    "        \n",
    "        # Fuse bottleneck features by concatenation\n",
    "        fused_bottleneck = torch.cat([rgb_bottleneck, depth_bottleneck], dim=1)\n",
    "        fused_bottleneck = self.attention(fused_bottleneck)\n",
    "        \n",
    "        # Fuse skip connections by concatenating corresponding features\n",
    "        fused_skips = [torch.cat([r, d], dim=1) for r, d in zip(rgb_skips, depth_skips)]\n",
    "        \n",
    "        # Decode to produce the inpainted RGB image\n",
    "        out = self.decoder(fused_bottleneck, fused_skips)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f46b53b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleInpaintingNet      : 33,711 parameters\n",
      "DepthSimpleAttention     : 89,107 parameters\n",
      "DepthMultiheadAttention  : 114,155 parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: torch.nn.Module) -> int:\n",
    "    \"\"\"Returns the number of trainable parameters in `model`.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# instantiate each\n",
    "models = {\n",
    "    \"SimpleInpaintingNet\": SimpleInpaintingNet(base_channels=8, levels=3),\n",
    "    \"DepthSimpleAttention\": DepthEnhancedInpaintingNet_SimpleAttention(base_channels=8, levels=3, reduction=4),\n",
    "    \"DepthMultiheadAttention\": DepthEnhancedInpaintingNet(base_channels=8, levels=3, num_heads=4),\n",
    "}\n",
    "\n",
    "for name, m in models.items():\n",
    "    print(f\"{name:25s}: {count_parameters(m):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad932e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
