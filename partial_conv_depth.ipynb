{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_pconv import PConv2d  # Ensure you have torch_pconv installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jinhyunpark/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 272\u001b[0m\n\u001b[1;32m    270\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 272\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, test_loader, criterion, device)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[3], line 252\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    250\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, mask)\n\u001b[1;32m    251\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m--> 252\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    254\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def pair_color_depth_files(root_dir):\n",
        "    color_dict = {}\n",
        "    depth_dict = {}\n",
        "    \n",
        "    for directory, _, files in os.walk(root_dir):\n",
        "        for f in files:\n",
        "            full_path = os.path.join(directory, f)\n",
        "            \n",
        "            # Determine a prefix based on whether we're in a subfolder or not.\n",
        "            if directory == root_dir:\n",
        "                prefix_prefix = \"\"  # Flat structure: don't add folder name.\n",
        "            else:\n",
        "                prefix_prefix = os.path.basename(directory) + \"_\"\n",
        "            \n",
        "            # Check file naming conventions and strip specific suffixes:\n",
        "            if f.endswith(\"_colors.png\"):\n",
        "                prefix = prefix_prefix + f.replace(\"_colors.png\", \"\")\n",
        "                color_dict[prefix] = full_path\n",
        "            elif f.endswith(\"_depth.png\"):\n",
        "                prefix = prefix_prefix + f.replace(\"_depth.png\", \"\")\n",
        "                depth_dict[prefix] = full_path\n",
        "            elif f.endswith(\".jpg\"):\n",
        "                # For jpg files assume they are color images.\n",
        "                prefix = prefix_prefix + f.replace(\".jpg\", \"\")\n",
        "                color_dict[prefix] = full_path\n",
        "            elif f.endswith(\".png\"):\n",
        "                # For png files not already handled, assume they are depth images.\n",
        "                prefix = prefix_prefix + f.replace(\".png\", \"\")\n",
        "                depth_dict[prefix] = full_path\n",
        "    \n",
        "    pairs = []\n",
        "    for prefix, cpath in color_dict.items():\n",
        "        if prefix in depth_dict:\n",
        "            pairs.append((cpath, depth_dict[prefix]))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Dataset: NYUDepthDataset with combined RGB+Depth\n",
        "# --------------------------------------------------\n",
        "class NYUDepthDataset(Dataset):\n",
        "    def __init__(self, root_dir, img_size=(240, 320), transform=None, apply_mask=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Path to the folder containing color/depth pairs.\n",
        "            img_size: Desired (height, width) for resizing.\n",
        "            transform: Optional transforms to be applied.\n",
        "            apply_mask: Whether to apply a random mask.\n",
        "        \"\"\"\n",
        "        super(NYUDepthDataset, self).__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.img_size = img_size  # e.g., (240, 320)\n",
        "        self.transform = transform\n",
        "        self.apply_mask = apply_mask\n",
        "\n",
        "        # Create a list of (color_path, depth_path) pairs.\n",
        "        self.samples = pair_color_depth_files(root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        color_path, depth_path = self.samples[idx]\n",
        "\n",
        "        # ----- Load color image -----\n",
        "        img = cv2.imread(color_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # Resize using (width, height)\n",
        "        img = cv2.resize(img, (self.img_size[1], self.img_size[0]))\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "\n",
        "        # ----- Load depth image -----\n",
        "        depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
        "        depth = cv2.resize(depth, (self.img_size[1], self.img_size[0]))\n",
        "        depth = depth.astype(np.float32) / 255.0\n",
        "\n",
        "        # ----- Apply mask if needed -----\n",
        "        if self.apply_mask:\n",
        "            mask_2d = self.create_random_mask(self.img_size)\n",
        "            # For the color image, create a 3-channel mask.\n",
        "            mask_3d = np.stack([mask_2d] * 3, axis=-1)\n",
        "\n",
        "            masked_img = img.copy()\n",
        "            masked_depth = depth.copy()\n",
        "\n",
        "            # Set masked regions to white for RGB and 0 for depth.\n",
        "            masked_img[mask_3d == 0] = 1.0\n",
        "            masked_depth[mask_2d == 0] = 0.0\n",
        "        else:\n",
        "            mask_2d = np.ones((self.img_size[0], self.img_size[1]), dtype=np.float32) * 255\n",
        "            masked_img = img\n",
        "            masked_depth = depth\n",
        "\n",
        "        # ----- Convert to Torch Tensors -----\n",
        "        # Color: (H, W, 3) -> (3, H, W)\n",
        "        img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "        masked_img_tensor = torch.from_numpy(masked_img).permute(2, 0, 1)\n",
        "        # Depth: (H, W) -> (1, H, W)\n",
        "        depth_tensor = torch.from_numpy(depth).unsqueeze(0)\n",
        "        masked_depth_tensor = torch.from_numpy(masked_depth).unsqueeze(0)\n",
        "        # Mask: (H, W) -> (1, H, W), normalized to [0,1]\n",
        "        mask_tensor = torch.from_numpy(mask_2d.astype(np.float32) / 255.0).unsqueeze(0)\n",
        "\n",
        "        # ----- Combine channels: create 4-channel tensors -----\n",
        "        # Combined masked input: (3+1, H, W)\n",
        "        combined_masked = torch.cat([masked_img_tensor, masked_depth_tensor], dim=0)\n",
        "        # Combined target: (3+1, H, W)\n",
        "        combined_target = torch.cat([img_tensor, depth_tensor], dim=0)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            combined_masked = self.transform(combined_masked)\n",
        "            combined_target = self.transform(combined_target)\n",
        "\n",
        "        return {\n",
        "            \"combined_masked\": combined_masked,\n",
        "            \"combined_target\": combined_target,\n",
        "            \"mask\": mask_tensor\n",
        "        }\n",
        "\n",
        "    def create_random_mask(self, size):\n",
        "        \"\"\"\n",
        "        Creates a random mask with white (255) for unmasked and black (0) for masked areas.\n",
        "        \"\"\"\n",
        "        H, W = size\n",
        "        mask = np.full((H, W), 255, np.uint8)\n",
        "        num_lines = np.random.randint(1, 10)\n",
        "        for _ in range(num_lines):\n",
        "            x1, x2 = np.random.randint(0, W, size=2)\n",
        "            y1, y2 = np.random.randint(0, H, size=2)\n",
        "            thickness = np.random.randint(1, 3)\n",
        "            cv2.line(mask, (x1, y1), (x2, y2), 0, thickness)\n",
        "        return mask\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Model: U-Net like Inpainting Model with Partial Convolutions\n",
        "# --------------------------------------------------\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.pconv1 = PConv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
        "                              stride=1, padding=padding, bias=True)\n",
        "        self.pconv2 = PConv2d(out_channels, out_channels, kernel_size=kernel_size, \n",
        "                              stride=2, padding=padding, bias=True)\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        # Ensure mask is (B, H, W)\n",
        "        if mask.dim() == 4 and mask.size(1) == 1:\n",
        "            mask = mask.squeeze(1)\n",
        "        x1, mask1 = self.pconv1(x, mask)\n",
        "        x1 = F.relu(x1)\n",
        "        x2, mask2 = self.pconv2(x1, mask1)\n",
        "        x2 = F.relu(x2)\n",
        "        return x1, mask1, x2, mask2\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels1, out_channels2, kernel_size=3, padding=1):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.pconv1 = PConv2d(skip_channels + in_channels, out_channels1, kernel_size=kernel_size, \n",
        "                              stride=1, padding=padding, bias=True)\n",
        "        self.pconv2 = PConv2d(out_channels1, out_channels2, kernel_size=kernel_size, \n",
        "                              stride=1, padding=padding, bias=True)\n",
        "    \n",
        "    def forward(self, x, mask, skip_x, skip_mask):\n",
        "        x_up = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        if mask.dim() == 4 and mask.size(1) == 1:\n",
        "            mask = mask.squeeze(1)\n",
        "        mask_up = F.interpolate(mask.unsqueeze(1), scale_factor=2, mode='nearest').squeeze(1)\n",
        "        if skip_mask.dim() == 4 and skip_mask.size(1) == 1:\n",
        "            skip_mask = skip_mask.squeeze(1)\n",
        "        # Combine masks using maximum (logical OR)\n",
        "        mask_cat = torch.max(skip_mask, mask_up)\n",
        "        x_cat = torch.cat([skip_x, x_up], dim=1)\n",
        "        x1, mask1 = self.pconv1(x_cat, mask_cat)\n",
        "        x1 = F.relu(x1)\n",
        "        x2, mask2 = self.pconv2(x1, mask1)\n",
        "        x2 = F.relu(x2)\n",
        "        return x1, mask1, x2, mask2\n",
        "\n",
        "class InpaintingModel(nn.Module):\n",
        "    def __init__(self, input_channels=4, output_channels=4):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_channels: Number of channels for the combined input (3 for RGB + 1 for depth).\n",
        "            output_channels: Number of channels for the combined output.\n",
        "        \"\"\"\n",
        "        super(InpaintingModel, self).__init__()\n",
        "        # Encoder\n",
        "        self.enc1 = EncoderBlock(input_channels, 32)\n",
        "        self.enc2 = EncoderBlock(32, 64)\n",
        "        self.enc3 = EncoderBlock(64, 128)\n",
        "        self.enc4 = EncoderBlock(128, 256)\n",
        "        \n",
        "        # Decoder with skip connections.\n",
        "        self.dec1 = DecoderBlock(256, 256, 256, 128)\n",
        "        self.dec2 = DecoderBlock(128, 128, 128, 64)\n",
        "        self.dec3 = DecoderBlock(64, 64, 64, 32)\n",
        "        self.dec4 = DecoderBlock(32, 32, 32, output_channels)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        if mask.dim() == 4 and mask.size(1) == 1:\n",
        "            mask = mask.squeeze(1)\n",
        "        conv1, mask1, conv2, mask2 = self.enc1(x, mask)\n",
        "        conv3, mask3, conv4, mask4 = self.enc2(conv2, mask2)\n",
        "        conv5, mask5, conv6, mask6 = self.enc3(conv4, mask4)\n",
        "        conv7, mask7, conv8, mask8 = self.enc4(conv6, mask6)\n",
        "        \n",
        "        conv9, mask9, conv10, mask10 = self.dec1(conv8, mask8, conv7, mask7)\n",
        "        conv11, mask11, conv12, mask12 = self.dec2(conv10, mask10, conv5, mask5)\n",
        "        conv13, mask13, conv14, mask14 = self.dec3(conv12, mask12, conv3, mask3)\n",
        "        conv15, mask15, conv16, mask16 = self.dec4(conv14, mask14, conv1, mask1)\n",
        "        \n",
        "        out = self.final_conv(conv16)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# DataLoader Setup\n",
        "# --------------------------------------------------\n",
        "# Replace these paths with your actual directories.\n",
        "train_data_path = \"nyu_data/data/nyu2_train\"\n",
        "test_data_path = \"nyu_data/data/nyu2_test\"\n",
        "\n",
        "# Create dataset instances.\n",
        "train_dataset = NYUDepthDataset(train_data_path, img_size=(240,320), apply_mask=True)\n",
        "test_dataset = NYUDepthDataset(test_data_path, img_size=(240,320), apply_mask=False)\n",
        "\n",
        "# Create DataLoaders.\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Training Setup\n",
        "# --------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = InpaintingModel(input_channels=4, output_channels=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        inputs = batch[\"combined_masked\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        targets = batch[\"combined_target\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, mask)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = batch[\"combined_masked\"].to(device)\n",
        "            mask = batch[\"mask\"].to(device)\n",
        "            targets = batch[\"combined_target\"].to(device)\n",
        "            outputs = model(inputs, mask)\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = validate(model, test_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"nyu_inpainting_depth_model.pth\")\n",
        "device = \"cpu\"\n",
        "model = InpaintingModel(input_channels=4).to(device)\n",
        "model.load_state_dict(torch.load(\"nyu_inpainting_depth_model.pth\", map_location=device))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Inference & Visualization\n",
        "# --------------------------------------------------\n",
        "num_samples = 32\n",
        "samples = [test_dataset[i] for i in range(num_samples)]\n",
        "combined_masked = torch.stack([s[\"combined_masked\"] for s in samples], dim=0)\n",
        "mask_tensor = torch.stack([s[\"mask\"] for s in samples], dim=0)\n",
        "combined_target = torch.stack([s[\"combined_target\"] for s in samples], dim=0)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for i in range(num_samples):\n",
        "        inp = combined_masked[i].unsqueeze(0).to(device)\n",
        "        msk = mask_tensor[i].unsqueeze(0).to(device)\n",
        "        pred = model(inp, msk)\n",
        "        predictions.append(pred.squeeze(0).cpu())\n",
        "\n",
        "# Visualization: show masked input, predicted output, and ground truth for RGB channels.\n",
        "fig, axs = plt.subplots(nrows=num_samples, ncols=3, figsize=(10, num_samples * 2))\n",
        "for i in range(num_samples):\n",
        "    # Extract the first 3 channels (RGB) for visualization.\n",
        "    masked_rgb = combined_masked[i][:3].permute(1, 2, 0).numpy()\n",
        "    pred_rgb = predictions[i][:3].permute(1, 2, 0).numpy()\n",
        "    target_rgb = combined_target[i][:3].permute(1, 2, 0).numpy()\n",
        "    \n",
        "    axs[i, 0].imshow(masked_rgb)\n",
        "    axs[i, 0].set_title(\"Masked Input (RGB)\")\n",
        "    axs[i, 0].axis(\"off\")\n",
        "    \n",
        "    axs[i, 1].imshow(pred_rgb)\n",
        "    axs[i, 1].set_title(\"Predicted Output (RGB)\")\n",
        "    axs[i, 1].axis(\"off\")\n",
        "    \n",
        "    axs[i, 2].imshow(target_rgb)\n",
        "    axs[i, 2].set_title(\"Ground Truth (RGB)\")\n",
        "    axs[i, 2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMbfu5iG1HOnVkbr6fZC+b9",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Image Inpainting Partial Convolution",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
