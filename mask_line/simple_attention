digraph {
	graph [size="71.25,71.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1971773314928 [label="
 (1, 3, 240, 320)" fillcolor=darkolivegreen1]
	1971762294032 [label=ConvolutionBackward0]
	1971762293984 -> 1971762294032
	1971762293984 [label=ReluBackward0]
	1971762296480 -> 1971762293984
	1971762296480 [label=NativeBatchNormBackward0]
	1971762297872 -> 1971762296480
	1971762297872 [label=ConvolutionBackward0]
	1971762296432 -> 1971762297872
	1971762296432 [label=ReluBackward0]
	1971762295664 -> 1971762296432
	1971762295664 [label=NativeBatchNormBackward0]
	1971762295760 -> 1971762295664
	1971762295760 [label=ConvolutionBackward0]
	1971762297296 -> 1971762295760
	1971762297296 [label=CatBackward0]
	1971761685040 -> 1971762297296
	1971761685040 [label=UpsampleBilinear2DBackward0]
	1971761814048 -> 1971761685040
	1971761814048 [label=ReluBackward0]
	1971761816352 -> 1971761814048
	1971761816352 [label=NativeBatchNormBackward0]
	1971761813184 -> 1971761816352
	1971761813184 [label=ConvolutionBackward0]
	1972895928032 -> 1971761813184
	1972895928032 [label=ReluBackward0]
	1971724668640 -> 1972895928032
	1971724668640 [label=NativeBatchNormBackward0]
	1971724668880 -> 1971724668640
	1971724668880 [label=ConvolutionBackward0]
	1971724670272 -> 1971724668880
	1971724670272 [label=CatBackward0]
	1971762299280 -> 1971724670272
	1971762299280 [label=UpsampleBilinear2DBackward0]
	1971762306960 -> 1971762299280
	1971762306960 [label=ReluBackward0]
	1971762299376 -> 1971762306960
	1971762299376 [label=NativeBatchNormBackward0]
	1971762307824 -> 1971762299376
	1971762307824 [label=ConvolutionBackward0]
	1971762307632 -> 1971762307824
	1971762307632 [label=ReluBackward0]
	1971762306672 -> 1971762307632
	1971762306672 [label=NativeBatchNormBackward0]
	1971762306240 -> 1971762306672
	1971762306240 [label=ConvolutionBackward0]
	1971762300864 -> 1971762306240
	1971762300864 [label=CatBackward0]
	1971762306384 -> 1971762300864
	1971762306384 [label=UpsampleBilinear2DBackward0]
	1971762306288 -> 1971762306384
	1971762306288 [label=AddBackward0]
	1971762305136 -> 1971762306288
	1971762305136 [label=MulBackward0]
	1971762307872 -> 1971762305136
	1971762307872 [label=MaxPool2DWithIndicesBackward0]
	1971762295808 -> 1971762307872
	1971762295808 [label=ReluBackward0]
	1971762309696 -> 1971762295808
	1971762309696 [label=NativeBatchNormBackward0]
	1971762310128 -> 1971762309696
	1971762310128 [label=ConvolutionBackward0]
	1971762303648 -> 1971762310128
	1971762303648 [label=ReluBackward0]
	1971762310320 -> 1971762303648
	1971762310320 [label=NativeBatchNormBackward0]
	1971762304800 -> 1971762310320
	1971762304800 [label=ConvolutionBackward0]
	1971762301920 -> 1971762304800
	1971762301920 [label=MaxPool2DWithIndicesBackward0]
	1971762300336 -> 1971762301920
	1971762300336 [label=ReluBackward0]
	1971762309264 -> 1971762300336
	1971762309264 [label=NativeBatchNormBackward0]
	1971762304608 -> 1971762309264
	1971762304608 [label=ConvolutionBackward0]
	1971762313104 -> 1971762304608
	1971762313104 [label=ReluBackward0]
	1971762313872 -> 1971762313104
	1971762313872 [label=NativeBatchNormBackward0]
	1971762313536 -> 1971762313872
	1971762313536 [label=ConvolutionBackward0]
	1971762313056 -> 1971762313536
	1971762313056 [label=MaxPool2DWithIndicesBackward0]
	1971762314832 -> 1971762313056
	1971762314832 [label=ReluBackward0]
	1971762314352 -> 1971762314832
	1971762314352 [label=NativeBatchNormBackward0]
	1971762314304 -> 1971762314352
	1971762314304 [label=ConvolutionBackward0]
	1971762314928 -> 1971762314304
	1971762314928 [label=ReluBackward0]
	1971762314544 -> 1971762314928
	1971762314544 [label=NativeBatchNormBackward0]
	1971762315024 -> 1971762314544
	1971762315024 [label=ConvolutionBackward0]
	1971762314592 -> 1971762315024
	1971724610736 [label="rgb_encoder.blocks.0.0.weight
 (8, 3, 3, 3)" fillcolor=lightblue]
	1971724610736 -> 1971762314592
	1971762314592 [label=AccumulateGrad]
	1971762315120 -> 1971762315024
	1971724610816 [label="rgb_encoder.blocks.0.0.bias
 (8)" fillcolor=lightblue]
	1971724610816 -> 1971762315120
	1971762315120 [label=AccumulateGrad]
	1971762314640 -> 1971762314544
	1971724611216 [label="rgb_encoder.blocks.0.1.weight
 (8)" fillcolor=lightblue]
	1971724611216 -> 1971762314640
	1971762314640 [label=AccumulateGrad]
	1971762314448 -> 1971762314544
	1971724615456 [label="rgb_encoder.blocks.0.1.bias
 (8)" fillcolor=lightblue]
	1971724615456 -> 1971762314448
	1971762314448 [label=AccumulateGrad]
	1971762314256 -> 1971762314304
	1971772542000 [label="rgb_encoder.blocks.0.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	1971772542000 -> 1971762314256
	1971762314256 [label=AccumulateGrad]
	1971762314160 -> 1971762314304
	1971772542480 [label="rgb_encoder.blocks.0.3.bias
 (8)" fillcolor=lightblue]
	1971772542480 -> 1971762314160
	1971762314160 [label=AccumulateGrad]
	1971762314688 -> 1971762314352
	1971772542320 [label="rgb_encoder.blocks.0.4.weight
 (8)" fillcolor=lightblue]
	1971772542320 -> 1971762314688
	1971762314688 [label=AccumulateGrad]
	1971762314112 -> 1971762314352
	1971772541600 [label="rgb_encoder.blocks.0.4.bias
 (8)" fillcolor=lightblue]
	1971772541600 -> 1971762314112
	1971762314112 [label=AccumulateGrad]
	1971762313728 -> 1971762313536
	1971772541680 [label="rgb_encoder.blocks.1.0.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	1971772541680 -> 1971762313728
	1971762313728 [label=AccumulateGrad]
	1971762313968 -> 1971762313536
	1971772543040 [label="rgb_encoder.blocks.1.0.bias
 (16)" fillcolor=lightblue]
	1971772543040 -> 1971762313968
	1971762313968 [label=AccumulateGrad]
	1971762313392 -> 1971762313872
	1971772542400 [label="rgb_encoder.blocks.1.1.weight
 (16)" fillcolor=lightblue]
	1971772542400 -> 1971762313392
	1971762313392 [label=AccumulateGrad]
	1971762313680 -> 1971762313872
	1971772543120 [label="rgb_encoder.blocks.1.1.bias
 (16)" fillcolor=lightblue]
	1971772543120 -> 1971762313680
	1971762313680 [label=AccumulateGrad]
	1971762313824 -> 1971762304608
	1971772551520 [label="rgb_encoder.blocks.1.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1971772551520 -> 1971762313824
	1971762313824 [label=AccumulateGrad]
	1971762303120 -> 1971762304608
	1971772551600 [label="rgb_encoder.blocks.1.3.bias
 (16)" fillcolor=lightblue]
	1971772551600 -> 1971762303120
	1971762303120 [label=AccumulateGrad]
	1971762309792 -> 1971762309264
	1971772551680 [label="rgb_encoder.blocks.1.4.weight
 (16)" fillcolor=lightblue]
	1971772551680 -> 1971762309792
	1971762309792 [label=AccumulateGrad]
	1971762304512 -> 1971762309264
	1971772539920 [label="rgb_encoder.blocks.1.4.bias
 (16)" fillcolor=lightblue]
	1971772539920 -> 1971762304512
	1971762304512 [label=AccumulateGrad]
	1971762309504 -> 1971762304800
	1971678779008 [label="rgb_encoder.blocks.2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	1971678779008 -> 1971762309504
	1971762309504 [label=AccumulateGrad]
	1971762311280 -> 1971762304800
	1971678785488 [label="rgb_encoder.blocks.2.0.bias
 (32)" fillcolor=lightblue]
	1971678785488 -> 1971762311280
	1971762311280 [label=AccumulateGrad]
	1971762304656 -> 1971762310320
	1971678779328 [label="rgb_encoder.blocks.2.1.weight
 (32)" fillcolor=lightblue]
	1971678779328 -> 1971762304656
	1971762304656 [label=AccumulateGrad]
	1971762309984 -> 1971762310320
	1971678785808 [label="rgb_encoder.blocks.2.1.bias
 (32)" fillcolor=lightblue]
	1971678785808 -> 1971762309984
	1971762309984 [label=AccumulateGrad]
	1971762304320 -> 1971762310128
	1971678779728 [label="rgb_encoder.blocks.2.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1971678779728 -> 1971762304320
	1971762304320 [label=AccumulateGrad]
	1971762310032 -> 1971762310128
	1971678779808 [label="rgb_encoder.blocks.2.3.bias
 (32)" fillcolor=lightblue]
	1971678779808 -> 1971762310032
	1971762310032 [label=AccumulateGrad]
	1971762300672 -> 1971762309696
	1971678786288 [label="rgb_encoder.blocks.2.4.weight
 (32)" fillcolor=lightblue]
	1971678786288 -> 1971762300672
	1971762300672 [label=AccumulateGrad]
	1971762305280 -> 1971762309696
	1971678786368 [label="rgb_encoder.blocks.2.4.bias
 (32)" fillcolor=lightblue]
	1971678786368 -> 1971762305280
	1971762305280 [label=AccumulateGrad]
	1971762299904 -> 1971762305136
	1971762299904 [label=SigmoidBackward0]
	1971762295472 -> 1971762299904
	1971762295472 [label=ConvolutionBackward0]
	1971762308016 -> 1971762295472
	1971762308016 [label=ReluBackward0]
	1971762304896 -> 1971762308016
	1971762304896 [label=ConvolutionBackward0]
	1971762306144 -> 1971762304896
	1971762306144 [label=CatBackward0]
	1971762307872 -> 1971762306144
	1971762305808 -> 1971762306144
	1971762305808 [label=MaxPool2DWithIndicesBackward0]
	1971762313776 -> 1971762305808
	1971762313776 [label=ReluBackward0]
	1971762314064 -> 1971762313776
	1971762314064 [label=NativeBatchNormBackward0]
	1971762313200 -> 1971762314064
	1971762313200 [label=ConvolutionBackward0]
	1971762314208 -> 1971762313200
	1971762314208 [label=ReluBackward0]
	1971762310368 -> 1971762314208
	1971762310368 [label=NativeBatchNormBackward0]
	1971762311232 -> 1971762310368
	1971762311232 [label=ConvolutionBackward0]
	1971762311136 -> 1971762311232
	1971762311136 [label=MaxPool2DWithIndicesBackward0]
	1971762311184 -> 1971762311136
	1971762311184 [label=ReluBackward0]
	1971762311088 -> 1971762311184
	1971762311088 [label=NativeBatchNormBackward0]
	1971762311952 -> 1971762311088
	1971762311952 [label=ConvolutionBackward0]
	1971762312000 -> 1971762311952
	1971762312000 [label=ReluBackward0]
	1971762307248 -> 1971762312000
	1971762307248 [label=NativeBatchNormBackward0]
	1971762307776 -> 1971762307248
	1971762307776 [label=ConvolutionBackward0]
	1971762307968 -> 1971762307776
	1971762307968 [label=MaxPool2DWithIndicesBackward0]
	1971762308160 -> 1971762307968
	1971762308160 [label=ReluBackward0]
	1971762307920 -> 1971762308160
	1971762307920 [label=NativeBatchNormBackward0]
	1971762308784 -> 1971762307920
	1971762308784 [label=ConvolutionBackward0]
	1971762309360 -> 1971762308784
	1971762309360 [label=ReluBackward0]
	1971762308544 -> 1971762309360
	1971762308544 [label=NativeBatchNormBackward0]
	1971762308592 -> 1971762308544
	1971762308592 [label=ConvolutionBackward0]
	1971762308352 -> 1971762308592
	1971678782368 [label="depth_encoder.blocks.0.0.weight
 (8, 1, 3, 3)" fillcolor=lightblue]
	1971678782368 -> 1971762308352
	1971762308352 [label=AccumulateGrad]
	1971762300816 -> 1971762308592
	1971678782528 [label="depth_encoder.blocks.0.0.bias
 (8)" fillcolor=lightblue]
	1971678782528 -> 1971762300816
	1971762300816 [label=AccumulateGrad]
	1971762301344 -> 1971762308544
	1971678782608 [label="depth_encoder.blocks.0.1.weight
 (8)" fillcolor=lightblue]
	1971678782608 -> 1971762301344
	1971762301344 [label=AccumulateGrad]
	1971762308064 -> 1971762308544
	1971678782448 [label="depth_encoder.blocks.0.1.bias
 (8)" fillcolor=lightblue]
	1971678782448 -> 1971762308064
	1971762308064 [label=AccumulateGrad]
	1971762308688 -> 1971762308784
	1971678807296 [label="depth_encoder.blocks.0.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	1971678807296 -> 1971762308688
	1971762308688 [label=AccumulateGrad]
	1971762308736 -> 1971762308784
	1971678806896 [label="depth_encoder.blocks.0.3.bias
 (8)" fillcolor=lightblue]
	1971678806896 -> 1971762308736
	1971762308736 [label=AccumulateGrad]
	1971762308496 -> 1971762307920
	1971678806976 [label="depth_encoder.blocks.0.4.weight
 (8)" fillcolor=lightblue]
	1971678806976 -> 1971762308496
	1971762308496 [label=AccumulateGrad]
	1971762307728 -> 1971762307920
	1971678807056 [label="depth_encoder.blocks.0.4.bias
 (8)" fillcolor=lightblue]
	1971678807056 -> 1971762307728
	1971762307728 [label=AccumulateGrad]
	1971762305664 -> 1971762307776
	1971678808176 [label="depth_encoder.blocks.1.0.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	1971678808176 -> 1971762305664
	1971762305664 [label=AccumulateGrad]
	1971762307488 -> 1971762307776
	1971678808256 [label="depth_encoder.blocks.1.0.bias
 (16)" fillcolor=lightblue]
	1971678808256 -> 1971762307488
	1971762307488 [label=AccumulateGrad]
	1971762308448 -> 1971762307248
	1971678808336 [label="depth_encoder.blocks.1.1.weight
 (16)" fillcolor=lightblue]
	1971678808336 -> 1971762308448
	1971762308448 [label=AccumulateGrad]
	1971762307104 -> 1971762307248
	1971678808416 [label="depth_encoder.blocks.1.1.bias
 (16)" fillcolor=lightblue]
	1971678808416 -> 1971762307104
	1971762307104 [label=AccumulateGrad]
	1971762312192 -> 1971762311952
	1971773314528 [label="depth_encoder.blocks.1.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1971773314528 -> 1971762312192
	1971762312192 [label=AccumulateGrad]
	1971762311568 -> 1971762311952
	1971773314208 [label="depth_encoder.blocks.1.3.bias
 (16)" fillcolor=lightblue]
	1971773314208 -> 1971762311568
	1971762311568 [label=AccumulateGrad]
	1971762311904 -> 1971762311088
	1971773315168 [label="depth_encoder.blocks.1.4.weight
 (16)" fillcolor=lightblue]
	1971773315168 -> 1971762311904
	1971762311904 [label=AccumulateGrad]
	1971762304416 -> 1971762311088
	1971773315088 [label="depth_encoder.blocks.1.4.bias
 (16)" fillcolor=lightblue]
	1971773315088 -> 1971762304416
	1971762304416 [label=AccumulateGrad]
	1971762310656 -> 1971762311232
	1971773587856 [label="depth_encoder.blocks.2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	1971773587856 -> 1971762310656
	1971762310656 [label=AccumulateGrad]
	1971762311328 -> 1971762311232
	1971773587936 [label="depth_encoder.blocks.2.0.bias
 (32)" fillcolor=lightblue]
	1971773587936 -> 1971762311328
	1971762311328 [label=AccumulateGrad]
	1971762311424 -> 1971762310368
	1971773588016 [label="depth_encoder.blocks.2.1.weight
 (32)" fillcolor=lightblue]
	1971773588016 -> 1971762311424
	1971762311424 [label=AccumulateGrad]
	1971762311616 -> 1971762310368
	1971773588096 [label="depth_encoder.blocks.2.1.bias
 (32)" fillcolor=lightblue]
	1971773588096 -> 1971762311616
	1971762311616 [label=AccumulateGrad]
	1971762310752 -> 1971762313200
	1971773588576 [label="depth_encoder.blocks.2.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1971773588576 -> 1971762310752
	1971762310752 [label=AccumulateGrad]
	1971762311760 -> 1971762313200
	1971773588656 [label="depth_encoder.blocks.2.3.bias
 (32)" fillcolor=lightblue]
	1971773588656 -> 1971762311760
	1971762311760 [label=AccumulateGrad]
	1971762312288 -> 1971762314064
	1971773588736 [label="depth_encoder.blocks.2.4.weight
 (32)" fillcolor=lightblue]
	1971773588736 -> 1971762312288
	1971762312288 [label=AccumulateGrad]
	1971762313632 -> 1971762314064
	1971773588816 [label="depth_encoder.blocks.2.4.bias
 (32)" fillcolor=lightblue]
	1971773588816 -> 1971762313632
	1971762313632 [label=AccumulateGrad]
	1971762304032 -> 1971762304896
	1971773589376 [label="fusion.conv1.weight
 (8, 64, 1, 1)" fillcolor=lightblue]
	1971773589376 -> 1971762304032
	1971762304032 [label=AccumulateGrad]
	1971762310176 -> 1971762304896
	1971773589456 [label="fusion.conv1.bias
 (8)" fillcolor=lightblue]
	1971773589456 -> 1971762310176
	1971762310176 [label=AccumulateGrad]
	1971762304272 -> 1971762295472
	1971773589616 [label="fusion.conv2.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1971773589616 -> 1971762304272
	1971762304272 [label=AccumulateGrad]
	1971762310416 -> 1971762295472
	1971773589696 [label="fusion.conv2.bias
 (32)" fillcolor=lightblue]
	1971773589696 -> 1971762310416
	1971762310416 [label=AccumulateGrad]
	1971762305808 -> 1971762306288
	1971762306048 -> 1971762300864
	1971762306048 [label=CatBackward0]
	1971762295808 -> 1971762306048
	1971762313776 -> 1971762306048
	1971762306096 -> 1971762306240
	1971773589776 [label="decoder.blocks.0.conv.0.weight
 (32, 96, 3, 3)" fillcolor=lightblue]
	1971773589776 -> 1971762306096
	1971762306096 [label=AccumulateGrad]
	1971762306624 -> 1971762306240
	1971773589856 [label="decoder.blocks.0.conv.0.bias
 (32)" fillcolor=lightblue]
	1971773589856 -> 1971762306624
	1971762306624 [label=AccumulateGrad]
	1971762306768 -> 1971762306672
	1971773589936 [label="decoder.blocks.0.conv.1.weight
 (32)" fillcolor=lightblue]
	1971773589936 -> 1971762306768
	1971762306768 [label=AccumulateGrad]
	1971762307536 -> 1971762306672
	1971773590016 [label="decoder.blocks.0.conv.1.bias
 (32)" fillcolor=lightblue]
	1971773590016 -> 1971762307536
	1971762307536 [label=AccumulateGrad]
	1971762300480 -> 1971762307824
	1971773590496 [label="decoder.blocks.0.conv.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1971773590496 -> 1971762300480
	1971762300480 [label=AccumulateGrad]
	1971762307392 -> 1971762307824
	1971773590576 [label="decoder.blocks.0.conv.3.bias
 (32)" fillcolor=lightblue]
	1971773590576 -> 1971762307392
	1971762307392 [label=AccumulateGrad]
	1971762303264 -> 1971762299376
	1971773590656 [label="decoder.blocks.0.conv.4.weight
 (32)" fillcolor=lightblue]
	1971773590656 -> 1971762303264
	1971762303264 [label=AccumulateGrad]
	1971762299184 -> 1971762299376
	1971773590736 [label="decoder.blocks.0.conv.4.bias
 (32)" fillcolor=lightblue]
	1971773590736 -> 1971762299184
	1971762299184 [label=AccumulateGrad]
	1971762299712 -> 1971724670272
	1971762299712 [label=CatBackward0]
	1971762300336 -> 1971762299712
	1971762311184 -> 1971762299712
	1971762310464 -> 1971724668880
	1971773591216 [label="decoder.blocks.1.conv.0.weight
 (16, 64, 3, 3)" fillcolor=lightblue]
	1971773591216 -> 1971762310464
	1971762310464 [label=AccumulateGrad]
	1971762301152 -> 1971724668880
	1971773591296 [label="decoder.blocks.1.conv.0.bias
 (16)" fillcolor=lightblue]
	1971773591296 -> 1971762301152
	1971762301152 [label=AccumulateGrad]
	1971724668976 -> 1971724668640
	1971773591376 [label="decoder.blocks.1.conv.1.weight
 (16)" fillcolor=lightblue]
	1971773591376 -> 1971724668976
	1971724668976 [label=AccumulateGrad]
	1971724669072 -> 1971724668640
	1971773591456 [label="decoder.blocks.1.conv.1.bias
 (16)" fillcolor=lightblue]
	1971773591456 -> 1971724669072
	1971724669072 [label=AccumulateGrad]
	1971727190816 -> 1971761813184
	1971773591936 [label="decoder.blocks.1.conv.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1971773591936 -> 1971727190816
	1971727190816 [label=AccumulateGrad]
	1971761801456 -> 1971761813184
	1971773592016 [label="decoder.blocks.1.conv.3.bias
 (16)" fillcolor=lightblue]
	1971773592016 -> 1971761801456
	1971761801456 [label=AccumulateGrad]
	1971761813616 -> 1971761816352
	1971773592096 [label="decoder.blocks.1.conv.4.weight
 (16)" fillcolor=lightblue]
	1971773592096 -> 1971761813616
	1971761813616 [label=AccumulateGrad]
	1971761813568 -> 1971761816352
	1971773592176 [label="decoder.blocks.1.conv.4.bias
 (16)" fillcolor=lightblue]
	1971773592176 -> 1971761813568
	1971761813568 [label=AccumulateGrad]
	1971761678896 -> 1971762297296
	1971761678896 [label=CatBackward0]
	1971762314832 -> 1971761678896
	1971762308160 -> 1971761678896
	1971762297056 -> 1971762295760
	1971773592576 [label="decoder.blocks.2.conv.0.weight
 (8, 32, 3, 3)" fillcolor=lightblue]
	1971773592576 -> 1971762297056
	1971762297056 [label=AccumulateGrad]
	1971762295136 -> 1971762295760
	1971773592656 [label="decoder.blocks.2.conv.0.bias
 (8)" fillcolor=lightblue]
	1971773592656 -> 1971762295136
	1971762295136 [label=AccumulateGrad]
	1971762296048 -> 1971762295664
	1971773592736 [label="decoder.blocks.2.conv.1.weight
 (8)" fillcolor=lightblue]
	1971773592736 -> 1971762296048
	1971762296048 [label=AccumulateGrad]
	1971762296000 -> 1971762295664
	1971773592816 [label="decoder.blocks.2.conv.1.bias
 (8)" fillcolor=lightblue]
	1971773592816 -> 1971762296000
	1971762296000 [label=AccumulateGrad]
	1971762296528 -> 1971762297872
	1971773593296 [label="decoder.blocks.2.conv.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	1971773593296 -> 1971762296528
	1971762296528 [label=AccumulateGrad]
	1971762298352 -> 1971762297872
	1971773593376 [label="decoder.blocks.2.conv.3.bias
 (8)" fillcolor=lightblue]
	1971773593376 -> 1971762298352
	1971762298352 [label=AccumulateGrad]
	1971762296576 -> 1971762296480
	1971773593456 [label="decoder.blocks.2.conv.4.weight
 (8)" fillcolor=lightblue]
	1971773593456 -> 1971762296576
	1971762296576 [label=AccumulateGrad]
	1971762293792 -> 1971762296480
	1971773593536 [label="decoder.blocks.2.conv.4.bias
 (8)" fillcolor=lightblue]
	1971773593536 -> 1971762293792
	1971762293792 [label=AccumulateGrad]
	1971762293888 -> 1971762294032
	1971773594016 [label="decoder.final_conv.weight
 (3, 8, 1, 1)" fillcolor=lightblue]
	1971773594016 -> 1971762293888
	1971762293888 [label=AccumulateGrad]
	1971762294176 -> 1971762294032
	1971773594096 [label="decoder.final_conv.bias
 (3)" fillcolor=lightblue]
	1971773594096 -> 1971762294176
	1971762294176 [label=AccumulateGrad]
	1971762294032 -> 1971773314928
}
