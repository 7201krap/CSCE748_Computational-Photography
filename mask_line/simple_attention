digraph {
	graph [size="71.25,71.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1659625188928 [label="
 (1, 3, 240, 320)" fillcolor=darkolivegreen1]
	1659623386720 [label=ConvolutionBackward0]
	1659623779408 -> 1659623386720
	1659623779408 [label=ReluBackward0]
	1659625109248 -> 1659623779408
	1659625109248 [label=NativeBatchNormBackward0]
	1659625109296 -> 1659625109248
	1659625109296 [label=ConvolutionBackward0]
	1659625109488 -> 1659625109296
	1659625109488 [label=ReluBackward0]
	1659625109680 -> 1659625109488
	1659625109680 [label=NativeBatchNormBackward0]
	1659625109776 -> 1659625109680
	1659625109776 [label=ConvolutionBackward0]
	1659625109968 -> 1659625109776
	1659625109968 [label=CatBackward0]
	1659625110160 -> 1659625109968
	1659625110160 [label=UpsampleBilinear2DBackward0]
	1659625110304 -> 1659625110160
	1659625110304 [label=ReluBackward0]
	1659625110400 -> 1659625110304
	1659625110400 [label=NativeBatchNormBackward0]
	1659625110496 -> 1659625110400
	1659625110496 [label=ConvolutionBackward0]
	1659625110688 -> 1659625110496
	1659625110688 [label=ReluBackward0]
	1659625110880 -> 1659625110688
	1659625110880 [label=NativeBatchNormBackward0]
	1659625110976 -> 1659625110880
	1659625110976 [label=ConvolutionBackward0]
	1659625111168 -> 1659625110976
	1659625111168 [label=CatBackward0]
	1659625111360 -> 1659625111168
	1659625111360 [label=UpsampleBilinear2DBackward0]
	1659625111504 -> 1659625111360
	1659625111504 [label=ReluBackward0]
	1659625111408 -> 1659625111504
	1659625111408 [label=NativeBatchNormBackward0]
	1659625423056 -> 1659625111408
	1659625423056 [label=ConvolutionBackward0]
	1659625423248 -> 1659625423056
	1659625423248 [label=ReluBackward0]
	1659625423440 -> 1659625423248
	1659625423440 [label=NativeBatchNormBackward0]
	1659625423536 -> 1659625423440
	1659625423536 [label=ConvolutionBackward0]
	1659625423728 -> 1659625423536
	1659625423728 [label=CatBackward0]
	1659625423920 -> 1659625423728
	1659625423920 [label=UpsampleBilinear2DBackward0]
	1659625424064 -> 1659625423920
	1659625424064 [label=AddBackward0]
	1659625424160 -> 1659625424064
	1659625424160 [label=MulBackward0]
	1659625424304 -> 1659625424160
	1659625424304 [label=MaxPool2DWithIndicesBackward0]
	1659625424448 -> 1659625424304
	1659625424448 [label=ReluBackward0]
	1659625424544 -> 1659625424448
	1659625424544 [label=NativeBatchNormBackward0]
	1659625424640 -> 1659625424544
	1659625424640 [label=ConvolutionBackward0]
	1659625424832 -> 1659625424640
	1659625424832 [label=ReluBackward0]
	1659625425024 -> 1659625424832
	1659625425024 [label=NativeBatchNormBackward0]
	1659625425120 -> 1659625425024
	1659625425120 [label=ConvolutionBackward0]
	1659625425312 -> 1659625425120
	1659625425312 [label=MaxPool2DWithIndicesBackward0]
	1659625425504 -> 1659625425312
	1659625425504 [label=ReluBackward0]
	1659625425600 -> 1659625425504
	1659625425600 [label=NativeBatchNormBackward0]
	1659625425696 -> 1659625425600
	1659625425696 [label=ConvolutionBackward0]
	1659625425888 -> 1659625425696
	1659625425888 [label=ReluBackward0]
	1659625426080 -> 1659625425888
	1659625426080 [label=NativeBatchNormBackward0]
	1659625426176 -> 1659625426080
	1659625426176 [label=ConvolutionBackward0]
	1659625426368 -> 1659625426176
	1659625426368 [label=MaxPool2DWithIndicesBackward0]
	1659625426560 -> 1659625426368
	1659625426560 [label=ReluBackward0]
	1659625426656 -> 1659625426560
	1659625426656 [label=NativeBatchNormBackward0]
	1659625426752 -> 1659625426656
	1659625426752 [label=ConvolutionBackward0]
	1659625426944 -> 1659625426752
	1659625426944 [label=ReluBackward0]
	1659625427136 -> 1659625426944
	1659625427136 [label=NativeBatchNormBackward0]
	1659625427232 -> 1659625427136
	1659625427232 [label=ConvolutionBackward0]
	1659625427424 -> 1659625427232
	1659559810528 [label="rgb_encoder.blocks.0.0.weight
 (8, 3, 3, 3)" fillcolor=lightblue]
	1659559810528 -> 1659625427424
	1659625427424 [label=AccumulateGrad]
	1659625427376 -> 1659625427232
	1659559810368 [label="rgb_encoder.blocks.0.0.bias
 (8)" fillcolor=lightblue]
	1659559810368 -> 1659625427376
	1659625427376 [label=AccumulateGrad]
	1659625427184 -> 1659625427136
	1659559809328 [label="rgb_encoder.blocks.0.1.weight
 (8)" fillcolor=lightblue]
	1659559809328 -> 1659625427184
	1659625427184 [label=AccumulateGrad]
	1659625427040 -> 1659625427136
	1659559809248 [label="rgb_encoder.blocks.0.1.bias
 (8)" fillcolor=lightblue]
	1659559809248 -> 1659625427040
	1659625427040 [label=AccumulateGrad]
	1659625426896 -> 1659625426752
	1659559807968 [label="rgb_encoder.blocks.0.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	1659559807968 -> 1659625426896
	1659625426896 [label=AccumulateGrad]
	1659625426848 -> 1659625426752
	1659559807888 [label="rgb_encoder.blocks.0.3.bias
 (8)" fillcolor=lightblue]
	1659559807888 -> 1659625426848
	1659625426848 [label=AccumulateGrad]
	1659625426704 -> 1659625426656
	1659559807808 [label="rgb_encoder.blocks.0.4.weight
 (8)" fillcolor=lightblue]
	1659559807808 -> 1659625426704
	1659625426704 [label=AccumulateGrad]
	1659625426464 -> 1659625426656
	1659559807728 [label="rgb_encoder.blocks.0.4.bias
 (8)" fillcolor=lightblue]
	1659559807728 -> 1659625426464
	1659625426464 [label=AccumulateGrad]
	1659625426320 -> 1659625426176
	1659559808128 [label="rgb_encoder.blocks.1.0.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	1659559808128 -> 1659625426320
	1659625426320 [label=AccumulateGrad]
	1659625426272 -> 1659625426176
	1659559805968 [label="rgb_encoder.blocks.1.0.bias
 (16)" fillcolor=lightblue]
	1659559805968 -> 1659625426272
	1659625426272 [label=AccumulateGrad]
	1659625426128 -> 1659625426080
	1659559805888 [label="rgb_encoder.blocks.1.1.weight
 (16)" fillcolor=lightblue]
	1659559805888 -> 1659625426128
	1659625426128 [label=AccumulateGrad]
	1659625425984 -> 1659625426080
	1659559812688 [label="rgb_encoder.blocks.1.1.bias
 (16)" fillcolor=lightblue]
	1659559812688 -> 1659625425984
	1659625425984 [label=AccumulateGrad]
	1659625425840 -> 1659625425696
	1659559782560 [label="rgb_encoder.blocks.1.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1659559782560 -> 1659625425840
	1659625425840 [label=AccumulateGrad]
	1659625425792 -> 1659625425696
	1659559773520 [label="rgb_encoder.blocks.1.3.bias
 (16)" fillcolor=lightblue]
	1659559773520 -> 1659625425792
	1659625425792 [label=AccumulateGrad]
	1659625425648 -> 1659625425600
	1659559773680 [label="rgb_encoder.blocks.1.4.weight
 (16)" fillcolor=lightblue]
	1659559773680 -> 1659625425648
	1659625425648 [label=AccumulateGrad]
	1659625425408 -> 1659625425600
	1659559773600 [label="rgb_encoder.blocks.1.4.bias
 (16)" fillcolor=lightblue]
	1659559773600 -> 1659625425408
	1659625425408 [label=AccumulateGrad]
	1659625425264 -> 1659625425120
	1659559782320 [label="rgb_encoder.blocks.2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	1659559782320 -> 1659625425264
	1659625425264 [label=AccumulateGrad]
	1659625425216 -> 1659625425120
	1659559782240 [label="rgb_encoder.blocks.2.0.bias
 (32)" fillcolor=lightblue]
	1659559782240 -> 1659625425216
	1659625425216 [label=AccumulateGrad]
	1659625425072 -> 1659625425024
	1659559773040 [label="rgb_encoder.blocks.2.1.weight
 (32)" fillcolor=lightblue]
	1659559773040 -> 1659625425072
	1659625425072 [label=AccumulateGrad]
	1659625424928 -> 1659625425024
	1659559773200 [label="rgb_encoder.blocks.2.1.bias
 (32)" fillcolor=lightblue]
	1659559773200 -> 1659625424928
	1659625424928 [label=AccumulateGrad]
	1659625424784 -> 1659625424640
	1659559772880 [label="rgb_encoder.blocks.2.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1659559772880 -> 1659625424784
	1659625424784 [label=AccumulateGrad]
	1659625424736 -> 1659625424640
	1659559782000 [label="rgb_encoder.blocks.2.3.bias
 (32)" fillcolor=lightblue]
	1659559782000 -> 1659625424736
	1659625424736 [label=AccumulateGrad]
	1659625424592 -> 1659625424544
	1659559781920 [label="rgb_encoder.blocks.2.4.weight
 (32)" fillcolor=lightblue]
	1659559781920 -> 1659625424592
	1659625424592 [label=AccumulateGrad]
	1659625424352 -> 1659625424544
	1659559772560 [label="rgb_encoder.blocks.2.4.bias
 (32)" fillcolor=lightblue]
	1659559772560 -> 1659625424352
	1659625424352 [label=AccumulateGrad]
	1659625424256 -> 1659625424160
	1659625424256 [label=SigmoidBackward0]
	1659598369456 -> 1659625424256
	1659598369456 [label=ConvolutionBackward0]
	1659625424880 -> 1659598369456
	1659625424880 [label=ReluBackward0]
	1659625425456 -> 1659625424880
	1659625425456 [label=ConvolutionBackward0]
	1659625425744 -> 1659625425456
	1659625425744 [label=CatBackward0]
	1659625424304 -> 1659625425744
	1659625424112 -> 1659625425744
	1659625424112 [label=MaxPool2DWithIndicesBackward0]
	1659625426512 -> 1659625424112
	1659625426512 [label=ReluBackward0]
	1659625426800 -> 1659625426512
	1659625426800 [label=NativeBatchNormBackward0]
	1659625427088 -> 1659625426800
	1659625427088 [label=ConvolutionBackward0]
	1659625427328 -> 1659625427088
	1659625427328 [label=ReluBackward0]
	1659625427712 -> 1659625427328
	1659625427712 [label=NativeBatchNormBackward0]
	1659625427808 -> 1659625427712
	1659625427808 [label=ConvolutionBackward0]
	1659625428000 -> 1659625427808
	1659625428000 [label=MaxPool2DWithIndicesBackward0]
	1659625428192 -> 1659625428000
	1659625428192 [label=ReluBackward0]
	1659625428288 -> 1659625428192
	1659625428288 [label=NativeBatchNormBackward0]
	1659625428384 -> 1659625428288
	1659625428384 [label=ConvolutionBackward0]
	1659625428576 -> 1659625428384
	1659625428576 [label=ReluBackward0]
	1659625428768 -> 1659625428576
	1659625428768 [label=NativeBatchNormBackward0]
	1659625428864 -> 1659625428768
	1659625428864 [label=ConvolutionBackward0]
	1659625429056 -> 1659625428864
	1659625429056 [label=MaxPool2DWithIndicesBackward0]
	1659625429248 -> 1659625429056
	1659625429248 [label=ReluBackward0]
	1659625429344 -> 1659625429248
	1659625429344 [label=NativeBatchNormBackward0]
	1659625429440 -> 1659625429344
	1659625429440 [label=ConvolutionBackward0]
	1659625429632 -> 1659625429440
	1659625429632 [label=ReluBackward0]
	1659625429824 -> 1659625429632
	1659625429824 [label=NativeBatchNormBackward0]
	1659625429920 -> 1659625429824
	1659625429920 [label=ConvolutionBackward0]
	1659625430112 -> 1659625429920
	1659559772320 [label="depth_encoder.blocks.0.0.weight
 (8, 1, 3, 3)" fillcolor=lightblue]
	1659559772320 -> 1659625430112
	1659625430112 [label=AccumulateGrad]
	1659625430064 -> 1659625429920
	1659559788400 [label="depth_encoder.blocks.0.0.bias
 (8)" fillcolor=lightblue]
	1659559788400 -> 1659625430064
	1659625430064 [label=AccumulateGrad]
	1659625429872 -> 1659625429824
	1659559788320 [label="depth_encoder.blocks.0.1.weight
 (8)" fillcolor=lightblue]
	1659559788320 -> 1659625429872
	1659625429872 [label=AccumulateGrad]
	1659625429728 -> 1659625429824
	1659559772480 [label="depth_encoder.blocks.0.1.bias
 (8)" fillcolor=lightblue]
	1659559772480 -> 1659625429728
	1659625429728 [label=AccumulateGrad]
	1659625429584 -> 1659625429440
	1659559781680 [label="depth_encoder.blocks.0.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	1659559781680 -> 1659625429584
	1659625429584 [label=AccumulateGrad]
	1659625429536 -> 1659625429440
	1659559781600 [label="depth_encoder.blocks.0.3.bias
 (8)" fillcolor=lightblue]
	1659559781600 -> 1659625429536
	1659625429536 [label=AccumulateGrad]
	1659625429392 -> 1659625429344
	1659559788240 [label="depth_encoder.blocks.0.4.weight
 (8)" fillcolor=lightblue]
	1659559788240 -> 1659625429392
	1659625429392 [label=AccumulateGrad]
	1659625429152 -> 1659625429344
	1659559788160 [label="depth_encoder.blocks.0.4.bias
 (8)" fillcolor=lightblue]
	1659559788160 -> 1659625429152
	1659625429152 [label=AccumulateGrad]
	1659625429008 -> 1659625428864
	1659559781200 [label="depth_encoder.blocks.1.0.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	1659559781200 -> 1659625429008
	1659625429008 [label=AccumulateGrad]
	1659625428960 -> 1659625428864
	1659559781120 [label="depth_encoder.blocks.1.0.bias
 (16)" fillcolor=lightblue]
	1659559781120 -> 1659625428960
	1659625428960 [label=AccumulateGrad]
	1659625428816 -> 1659625428768
	1659559787760 [label="depth_encoder.blocks.1.1.weight
 (16)" fillcolor=lightblue]
	1659559787760 -> 1659625428816
	1659625428816 [label=AccumulateGrad]
	1659625428672 -> 1659625428768
	1659559787680 [label="depth_encoder.blocks.1.1.bias
 (16)" fillcolor=lightblue]
	1659559787680 -> 1659625428672
	1659625428672 [label=AccumulateGrad]
	1659625428528 -> 1659625428384
	1659559780800 [label="depth_encoder.blocks.1.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1659559780800 -> 1659625428528
	1659625428528 [label=AccumulateGrad]
	1659625428480 -> 1659625428384
	1659559787440 [label="depth_encoder.blocks.1.3.bias
 (16)" fillcolor=lightblue]
	1659559787440 -> 1659625428480
	1659625428480 [label=AccumulateGrad]
	1659625428336 -> 1659625428288
	1659559787360 [label="depth_encoder.blocks.1.4.weight
 (16)" fillcolor=lightblue]
	1659559787360 -> 1659625428336
	1659625428336 [label=AccumulateGrad]
	1659625428096 -> 1659625428288
	1659559780720 [label="depth_encoder.blocks.1.4.bias
 (16)" fillcolor=lightblue]
	1659559780720 -> 1659625428096
	1659625428096 [label=AccumulateGrad]
	1659625427952 -> 1659625427808
	1659559787120 [label="depth_encoder.blocks.2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	1659559787120 -> 1659625427952
	1659625427952 [label=AccumulateGrad]
	1659625427904 -> 1659625427808
	1659559787040 [label="depth_encoder.blocks.2.0.bias
 (32)" fillcolor=lightblue]
	1659559787040 -> 1659625427904
	1659625427904 [label=AccumulateGrad]
	1659625427760 -> 1659625427712
	1659559780400 [label="depth_encoder.blocks.2.1.weight
 (32)" fillcolor=lightblue]
	1659559780400 -> 1659625427760
	1659625427760 [label=AccumulateGrad]
	1659625427616 -> 1659625427712
	1659559780320 [label="depth_encoder.blocks.2.1.bias
 (32)" fillcolor=lightblue]
	1659559780320 -> 1659625427616
	1659625427616 [label=AccumulateGrad]
	1659625427568 -> 1659625427088
	1659559786720 [label="depth_encoder.blocks.2.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1659559786720 -> 1659625427568
	1659625427568 [label=AccumulateGrad]
	1659625427472 -> 1659625427088
	1659559780080 [label="depth_encoder.blocks.2.3.bias
 (32)" fillcolor=lightblue]
	1659559780080 -> 1659625427472
	1659625427472 [label=AccumulateGrad]
	1659625426992 -> 1659625426800
	1659559780000 [label="depth_encoder.blocks.2.4.weight
 (32)" fillcolor=lightblue]
	1659559780000 -> 1659625426992
	1659625426992 [label=AccumulateGrad]
	1659625426224 -> 1659625426800
	1659559786640 [label="depth_encoder.blocks.2.4.bias
 (32)" fillcolor=lightblue]
	1659559786640 -> 1659625426224
	1659625426224 [label=AccumulateGrad]
	1659625425552 -> 1659625425456
	1659559779680 [label="fusion.conv1.weight
 (8, 64, 1, 1)" fillcolor=lightblue]
	1659559779680 -> 1659625425552
	1659625425552 [label=AccumulateGrad]
	1659625425168 -> 1659625425456
	1659559786320 [label="fusion.conv1.bias
 (8)" fillcolor=lightblue]
	1659559786320 -> 1659625425168
	1659625425168 [label=AccumulateGrad]
	1659625424688 -> 1659598369456
	1659559779600 [label="fusion.conv2.weight
 (32, 8, 1, 1)" fillcolor=lightblue]
	1659559779600 -> 1659625424688
	1659625424688 [label=AccumulateGrad]
	1659625424400 -> 1659598369456
	1659559779520 [label="fusion.conv2.bias
 (32)" fillcolor=lightblue]
	1659559779520 -> 1659625424400
	1659625424400 [label=AccumulateGrad]
	1659625424112 -> 1659625424064
	1659625423872 -> 1659625423728
	1659625423872 [label=CatBackward0]
	1659625424448 -> 1659625423872
	1659625426512 -> 1659625423872
	1659625423680 -> 1659625423536
	1659559785840 [label="decoder.blocks.0.conv.0.weight
 (32, 96, 3, 3)" fillcolor=lightblue]
	1659559785840 -> 1659625423680
	1659625423680 [label=AccumulateGrad]
	1659625423632 -> 1659625423536
	1659559785760 [label="decoder.blocks.0.conv.0.bias
 (32)" fillcolor=lightblue]
	1659559785760 -> 1659625423632
	1659625423632 [label=AccumulateGrad]
	1659625423488 -> 1659625423440
	1659559779440 [label="decoder.blocks.0.conv.1.weight
 (32)" fillcolor=lightblue]
	1659559779440 -> 1659625423488
	1659625423488 [label=AccumulateGrad]
	1659625423344 -> 1659625423440
	1659559779360 [label="decoder.blocks.0.conv.1.bias
 (32)" fillcolor=lightblue]
	1659559779360 -> 1659625423344
	1659625423344 [label=AccumulateGrad]
	1659625423200 -> 1659625423056
	1659559785600 [label="decoder.blocks.0.conv.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1659559785600 -> 1659625423200
	1659625423200 [label=AccumulateGrad]
	1659625423152 -> 1659625423056
	1659559778960 [label="decoder.blocks.0.conv.3.bias
 (32)" fillcolor=lightblue]
	1659559778960 -> 1659625423152
	1659625423152 [label=AccumulateGrad]
	1659625423008 -> 1659625111408
	1659559778880 [label="decoder.blocks.0.conv.4.weight
 (32)" fillcolor=lightblue]
	1659559778880 -> 1659625423008
	1659625423008 [label=AccumulateGrad]
	1659625422912 -> 1659625111408
	1659559785520 [label="decoder.blocks.0.conv.4.bias
 (32)" fillcolor=lightblue]
	1659559785520 -> 1659625422912
	1659625422912 [label=AccumulateGrad]
	1659625111312 -> 1659625111168
	1659625111312 [label=CatBackward0]
	1659625425504 -> 1659625111312
	1659625428192 -> 1659625111312
	1659625111120 -> 1659625110976
	1659559778640 [label="decoder.blocks.1.conv.0.weight
 (16, 64, 3, 3)" fillcolor=lightblue]
	1659559778640 -> 1659625111120
	1659625111120 [label=AccumulateGrad]
	1659625111072 -> 1659625110976
	1659559778560 [label="decoder.blocks.1.conv.0.bias
 (16)" fillcolor=lightblue]
	1659559778560 -> 1659625111072
	1659625111072 [label=AccumulateGrad]
	1659625110928 -> 1659625110880
	1659559785200 [label="decoder.blocks.1.conv.1.weight
 (16)" fillcolor=lightblue]
	1659559785200 -> 1659625110928
	1659625110928 [label=AccumulateGrad]
	1659625110784 -> 1659625110880
	1659559785120 [label="decoder.blocks.1.conv.1.bias
 (16)" fillcolor=lightblue]
	1659559785120 -> 1659625110784
	1659625110784 [label=AccumulateGrad]
	1659625110640 -> 1659625110496
	1659559778080 [label="decoder.blocks.1.conv.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1659559778080 -> 1659625110640
	1659625110640 [label=AccumulateGrad]
	1659625110592 -> 1659625110496
	1659559784720 [label="decoder.blocks.1.conv.3.bias
 (16)" fillcolor=lightblue]
	1659559784720 -> 1659625110592
	1659625110592 [label=AccumulateGrad]
	1659625110448 -> 1659625110400
	1659559784640 [label="decoder.blocks.1.conv.4.weight
 (16)" fillcolor=lightblue]
	1659559784640 -> 1659625110448
	1659625110448 [label=AccumulateGrad]
	1659625110208 -> 1659625110400
	1659559778000 [label="decoder.blocks.1.conv.4.bias
 (16)" fillcolor=lightblue]
	1659559778000 -> 1659625110208
	1659625110208 [label=AccumulateGrad]
	1659625110112 -> 1659625109968
	1659625110112 [label=CatBackward0]
	1659625426560 -> 1659625110112
	1659625429248 -> 1659625110112
	1659625109920 -> 1659625109776
	1659559784400 [label="decoder.blocks.2.conv.0.weight
 (8, 32, 3, 3)" fillcolor=lightblue]
	1659559784400 -> 1659625109920
	1659625109920 [label=AccumulateGrad]
	1659625109872 -> 1659625109776
	1659559784320 [label="decoder.blocks.2.conv.0.bias
 (8)" fillcolor=lightblue]
	1659559784320 -> 1659625109872
	1659625109872 [label=AccumulateGrad]
	1659625109728 -> 1659625109680
	1659559777680 [label="decoder.blocks.2.conv.1.weight
 (8)" fillcolor=lightblue]
	1659559777680 -> 1659625109728
	1659625109728 [label=AccumulateGrad]
	1659625109584 -> 1659625109680
	1659559777600 [label="decoder.blocks.2.conv.1.bias
 (8)" fillcolor=lightblue]
	1659559777600 -> 1659625109584
	1659625109584 [label=AccumulateGrad]
	1659625109440 -> 1659625109296
	1659559784080 [label="decoder.blocks.2.conv.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	1659559784080 -> 1659625109440
	1659625109440 [label=AccumulateGrad]
	1659625109392 -> 1659625109296
	1659559784000 [label="decoder.blocks.2.conv.3.bias
 (8)" fillcolor=lightblue]
	1659559784000 -> 1659625109392
	1659625109392 [label=AccumulateGrad]
	1659625109344 -> 1659625109248
	1659559778480 [label="decoder.blocks.2.conv.4.weight
 (8)" fillcolor=lightblue]
	1659559778480 -> 1659625109344
	1659625109344 [label=AccumulateGrad]
	1659625108960 -> 1659625109248
	1659559778400 [label="decoder.blocks.2.conv.4.bias
 (8)" fillcolor=lightblue]
	1659559778400 -> 1659625108960
	1659625108960 [label=AccumulateGrad]
	1659625109152 -> 1659623386720
	1659559773920 [label="decoder.final_conv.weight
 (3, 8, 1, 1)" fillcolor=lightblue]
	1659559773920 -> 1659625109152
	1659625109152 [label=AccumulateGrad]
	1659625109104 -> 1659623386720
	1659559773840 [label="decoder.final_conv.bias
 (3)" fillcolor=lightblue]
	1659559773840 -> 1659625109104
	1659625109104 [label=AccumulateGrad]
	1659623386720 -> 1659625188928
}
