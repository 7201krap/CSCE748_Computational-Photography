digraph {
	graph [size="80.25,80.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2453085074320 [label="
 (1, 3, 240, 320)" fillcolor=darkolivegreen1]
	2452868070128 [label=ConvolutionBackward0]
	2453065772112 -> 2452868070128
	2453065772112 [label=ReluBackward0]
	2453067098352 -> 2453065772112
	2453067098352 [label=NativeBatchNormBackward0]
	2453084926160 -> 2453067098352
	2453084926160 [label=ConvolutionBackward0]
	2453084926256 -> 2453084926160
	2453084926256 [label=ReluBackward0]
	2453084926448 -> 2453084926256
	2453084926448 [label=NativeBatchNormBackward0]
	2453084926544 -> 2453084926448
	2453084926544 [label=ConvolutionBackward0]
	2453084926736 -> 2453084926544
	2453084926736 [label=CatBackward0]
	2453084926928 -> 2453084926736
	2453084926928 [label=UpsampleBilinear2DBackward0]
	2453066230960 -> 2453084926928
	2453066230960 [label=ReluBackward0]
	2453084927120 -> 2453066230960
	2453084927120 [label=NativeBatchNormBackward0]
	2453084927216 -> 2453084927120
	2453084927216 [label=ConvolutionBackward0]
	2453084927408 -> 2453084927216
	2453084927408 [label=ReluBackward0]
	2453084927600 -> 2453084927408
	2453084927600 [label=NativeBatchNormBackward0]
	2453084927696 -> 2453084927600
	2453084927696 [label=ConvolutionBackward0]
	2453084927888 -> 2453084927696
	2453084927888 [label=CatBackward0]
	2453084928080 -> 2453084927888
	2453084928080 [label=UpsampleBilinear2DBackward0]
	2453084928224 -> 2453084928080
	2453084928224 [label=ReluBackward0]
	2453084928320 -> 2453084928224
	2453084928320 [label=NativeBatchNormBackward0]
	2453084928416 -> 2453084928320
	2453084928416 [label=ConvolutionBackward0]
	2453084928608 -> 2453084928416
	2453084928608 [label=ReluBackward0]
	2453084928800 -> 2453084928608
	2453084928800 [label=NativeBatchNormBackward0]
	2453084928896 -> 2453084928800
	2453084928896 [label=ConvolutionBackward0]
	2453084929088 -> 2453084928896
	2453084929088 [label=CatBackward0]
	2453084929280 -> 2453084929088
	2453084929280 [label=UpsampleBilinear2DBackward0]
	2453084929424 -> 2453084929280
	2453084929424 [label=ViewBackward0]
	2453084929520 -> 2453084929424
	2453084929520 [label=PermuteBackward0]
	2453084929616 -> 2453084929520
	2453084929616 [label=TransposeBackward0]
	2453084929712 -> 2453084929616
	2453084929712 [label=ViewBackward0]
	2453084929808 -> 2453084929712
	2453084929808 [label=AddmmBackward0]
	2453084929904 -> 2453084929808
	2453019629264 [label="attention.mha.out_proj.bias
 (64)" fillcolor=lightblue]
	2453019629264 -> 2453084929904
	2453084929904 [label=AccumulateGrad]
	2453084929856 -> 2453084929808
	2453084929856 [label=ViewBackward0]
	2453084930000 -> 2453084929856
	2453084930000 [label=CloneBackward0]
	2453085307040 -> 2453084930000
	2453085307040 [label=TransposeBackward0]
	2453085307136 -> 2453085307040
	2453085307136 [label=BmmBackward0]
	2453085307232 -> 2453085307136
	2453085307232 [label=SoftmaxBackward0]
	2453085307376 -> 2453085307232
	2453085307376 [label=BmmBackward0]
	2453085307472 -> 2453085307376
	2453085307472 [label=DivBackward0]
	2453085307616 -> 2453085307472
	2453085307616 [label=TransposeBackward0]
	2453085307712 -> 2453085307616
	2453085307712 [label=ViewBackward0]
	2453085307808 -> 2453085307712
	2453085307808 [label=SelectBackward0]
	2453085307904 -> 2453085307808
	2453085307904 [label=CloneBackward0]
	2453085308000 -> 2453085307904
	2453085308000 [label=SqueezeBackward1]
	2453085308096 -> 2453085308000
	2453085308096 [label=TransposeBackward0]
	2453085308192 -> 2453085308096
	2453085308192 [label=UnsqueezeBackward0]
	2453085308288 -> 2453085308192
	2453085308288 [label=ViewBackward0]
	2453085308336 -> 2453085308288
	2453085308336 [label=AddBackward0]
	2453085308480 -> 2453085308336
	2453085308480 [label=UnsafeViewBackward0]
	2453085308720 -> 2453085308480
	2453085308720 [label=MmBackward0]
	2453085308768 -> 2453085308720
	2453085308768 [label=ReshapeAliasBackward0]
	2453085309008 -> 2453085308768
	2453085309008 [label=TransposeBackward0]
	2453085309056 -> 2453085309008
	2453085309056 [label=PermuteBackward0]
	2453085309200 -> 2453085309056
	2453085309200 [label=ViewBackward0]
	2453085309344 -> 2453085309200
	2453085309344 [label=CatBackward0]
	2453085309488 -> 2453085309344
	2453085309488 [label=MaxPool2DWithIndicesBackward0]
	2453085309728 -> 2453085309488
	2453085309728 [label=ReluBackward0]
	2453085309776 -> 2453085309728
	2453085309776 [label=NativeBatchNormBackward0]
	2453085309920 -> 2453085309776
	2453085309920 [label=ConvolutionBackward0]
	2453085310208 -> 2453085309920
	2453085310208 [label=ReluBackward0]
	2453085310400 -> 2453085310208
	2453085310400 [label=NativeBatchNormBackward0]
	2453085310448 -> 2453085310400
	2453085310448 [label=ConvolutionBackward0]
	2453085310736 -> 2453085310448
	2453085310736 [label=MaxPool2DWithIndicesBackward0]
	2453085310928 -> 2453085310736
	2453085310928 [label=ReluBackward0]
	2453085310976 -> 2453085310928
	2453085310976 [label=NativeBatchNormBackward0]
	2453085311120 -> 2453085310976
	2453085311120 [label=ConvolutionBackward0]
	2453085311408 -> 2453085311120
	2453085311408 [label=ReluBackward0]
	2453085311600 -> 2453085311408
	2453085311600 [label=NativeBatchNormBackward0]
	2453085311648 -> 2453085311600
	2453085311648 [label=ConvolutionBackward0]
	2453085311936 -> 2453085311648
	2453085311936 [label=MaxPool2DWithIndicesBackward0]
	2453085312128 -> 2453085311936
	2453085312128 [label=ReluBackward0]
	2453085312176 -> 2453085312128
	2453085312176 [label=NativeBatchNormBackward0]
	2453085312320 -> 2453085312176
	2453085312320 [label=ConvolutionBackward0]
	2453085312608 -> 2453085312320
	2453085312608 [label=ReluBackward0]
	2453085312800 -> 2453085312608
	2453085312800 [label=NativeBatchNormBackward0]
	2453085312848 -> 2453085312800
	2453085312848 [label=ConvolutionBackward0]
	2453085313136 -> 2453085312848
	2453019616240 [label="rgb_encoder.blocks.0.0.weight
 (8, 3, 3, 3)" fillcolor=lightblue]
	2453019616240 -> 2453085313136
	2453085313136 [label=AccumulateGrad]
	2453085313088 -> 2453085312848
	2453019622720 [label="rgb_encoder.blocks.0.0.bias
 (8)" fillcolor=lightblue]
	2453019622720 -> 2453085313088
	2453085313088 [label=AccumulateGrad]
	2453085312704 -> 2453085312800
	2453019622320 [label="rgb_encoder.blocks.0.1.weight
 (8)" fillcolor=lightblue]
	2453019622320 -> 2453085312704
	2453085312704 [label=AccumulateGrad]
	2453085312944 -> 2453085312800
	2453019615840 [label="rgb_encoder.blocks.0.1.bias
 (8)" fillcolor=lightblue]
	2453019615840 -> 2453085312944
	2453085312944 [label=AccumulateGrad]
	2453085312560 -> 2453085312320
	2453019621600 [label="rgb_encoder.blocks.0.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	2453019621600 -> 2453085312560
	2453085312560 [label=AccumulateGrad]
	2453085312512 -> 2453085312320
	2453019621520 [label="rgb_encoder.blocks.0.3.bias
 (8)" fillcolor=lightblue]
	2453019621520 -> 2453085312512
	2453085312512 [label=AccumulateGrad]
	2453085312272 -> 2453085312176
	2453019615040 [label="rgb_encoder.blocks.0.4.weight
 (8)" fillcolor=lightblue]
	2453019615040 -> 2453085312272
	2453085312272 [label=AccumulateGrad]
	2453085312416 -> 2453085312176
	2453019614960 [label="rgb_encoder.blocks.0.4.bias
 (8)" fillcolor=lightblue]
	2453019614960 -> 2453085312416
	2453085312416 [label=AccumulateGrad]
	2453085311888 -> 2453085311648
	2453019621120 [label="rgb_encoder.blocks.1.0.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	2453019621120 -> 2453085311888
	2453085311888 [label=AccumulateGrad]
	2453085311840 -> 2453085311648
	2453019621040 [label="rgb_encoder.blocks.1.0.bias
 (16)" fillcolor=lightblue]
	2453019621040 -> 2453085311840
	2453085311840 [label=AccumulateGrad]
	2453085311504 -> 2453085311600
	2453019614560 [label="rgb_encoder.blocks.1.1.weight
 (16)" fillcolor=lightblue]
	2453019614560 -> 2453085311504
	2453085311504 [label=AccumulateGrad]
	2453085311744 -> 2453085311600
	2453019614480 [label="rgb_encoder.blocks.1.1.bias
 (16)" fillcolor=lightblue]
	2453019614480 -> 2453085311744
	2453085311744 [label=AccumulateGrad]
	2453085311360 -> 2453085311120
	2453019620720 [label="rgb_encoder.blocks.1.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	2453019620720 -> 2453085311360
	2453085311360 [label=AccumulateGrad]
	2453085311312 -> 2453085311120
	2453019614240 [label="rgb_encoder.blocks.1.3.bias
 (16)" fillcolor=lightblue]
	2453019614240 -> 2453085311312
	2453085311312 [label=AccumulateGrad]
	2453085311072 -> 2453085310976
	2453019614160 [label="rgb_encoder.blocks.1.4.weight
 (16)" fillcolor=lightblue]
	2453019614160 -> 2453085311072
	2453085311072 [label=AccumulateGrad]
	2453085311216 -> 2453085310976
	2453019620640 [label="rgb_encoder.blocks.1.4.bias
 (16)" fillcolor=lightblue]
	2453019620640 -> 2453085311216
	2453085311216 [label=AccumulateGrad]
	2453085310688 -> 2453085310448
	2453019613920 [label="rgb_encoder.blocks.2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	2453019613920 -> 2453085310688
	2453085310688 [label=AccumulateGrad]
	2453085310640 -> 2453085310448
	2453019613840 [label="rgb_encoder.blocks.2.0.bias
 (32)" fillcolor=lightblue]
	2453019613840 -> 2453085310640
	2453085310640 [label=AccumulateGrad]
	2453085310304 -> 2453085310400
	2453019620320 [label="rgb_encoder.blocks.2.1.weight
 (32)" fillcolor=lightblue]
	2453019620320 -> 2453085310304
	2453085310304 [label=AccumulateGrad]
	2453085310544 -> 2453085310400
	2453019620240 [label="rgb_encoder.blocks.2.1.bias
 (32)" fillcolor=lightblue]
	2453019620240 -> 2453085310544
	2453085310544 [label=AccumulateGrad]
	2453085310160 -> 2453085309920
	2453019613360 [label="rgb_encoder.blocks.2.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2453019613360 -> 2453085310160
	2453085310160 [label=AccumulateGrad]
	2453085310112 -> 2453085309920
	2453019620160 [label="rgb_encoder.blocks.2.3.bias
 (32)" fillcolor=lightblue]
	2453019620160 -> 2453085310112
	2453085310112 [label=AccumulateGrad]
	2453085309872 -> 2453085309776
	2453019620080 [label="rgb_encoder.blocks.2.4.weight
 (32)" fillcolor=lightblue]
	2453019620080 -> 2453085309872
	2453085309872 [label=AccumulateGrad]
	2453085310016 -> 2453085309776
	2453019613600 [label="rgb_encoder.blocks.2.4.bias
 (32)" fillcolor=lightblue]
	2453019613600 -> 2453085310016
	2453085310016 [label=AccumulateGrad]
	2453085309440 -> 2453085309344
	2453085309440 [label=MaxPool2DWithIndicesBackward0]
	2453085310064 -> 2453085309440
	2453085310064 [label=ReluBackward0]
	2453085310352 -> 2453085310064
	2453085310352 [label=NativeBatchNormBackward0]
	2453085310832 -> 2453085310352
	2453085310832 [label=ConvolutionBackward0]
	2453085311792 -> 2453085310832
	2453085311792 [label=ReluBackward0]
	2453085312464 -> 2453085311792
	2453085312464 [label=NativeBatchNormBackward0]
	2453085312080 -> 2453085312464
	2453085312080 [label=ConvolutionBackward0]
	2453085312992 -> 2453085312080
	2453085312992 [label=MaxPool2DWithIndicesBackward0]
	2453085313328 -> 2453085312992
	2453085313328 [label=ReluBackward0]
	2453085313424 -> 2453085313328
	2453085313424 [label=NativeBatchNormBackward0]
	2453085313520 -> 2453085313424
	2453085313520 [label=ConvolutionBackward0]
	2453085313712 -> 2453085313520
	2453085313712 [label=ReluBackward0]
	2453085313904 -> 2453085313712
	2453085313904 [label=NativeBatchNormBackward0]
	2453085314000 -> 2453085313904
	2453085314000 [label=ConvolutionBackward0]
	2453085314192 -> 2453085314000
	2453085314192 [label=MaxPool2DWithIndicesBackward0]
	2453085314384 -> 2453085314192
	2453085314384 [label=ReluBackward0]
	2453085314480 -> 2453085314384
	2453085314480 [label=NativeBatchNormBackward0]
	2453085314576 -> 2453085314480
	2453085314576 [label=ConvolutionBackward0]
	2453085314768 -> 2453085314576
	2453085314768 [label=ReluBackward0]
	2453085314960 -> 2453085314768
	2453085314960 [label=NativeBatchNormBackward0]
	2453085315104 -> 2453085314960
	2453085315104 [label=ConvolutionBackward0]
	2453085315248 -> 2453085315104
	2453019619600 [label="depth_encoder.blocks.0.0.weight
 (8, 1, 3, 3)" fillcolor=lightblue]
	2453019619600 -> 2453085315248
	2453085315248 [label=AccumulateGrad]
	2453085315200 -> 2453085315104
	2453019613120 [label="depth_encoder.blocks.0.0.bias
 (8)" fillcolor=lightblue]
	2453019613120 -> 2453085315200
	2453085315200 [label=AccumulateGrad]
	2453085315008 -> 2453085314960
	2453019613040 [label="depth_encoder.blocks.0.1.weight
 (8)" fillcolor=lightblue]
	2453019613040 -> 2453085315008
	2453085315008 [label=AccumulateGrad]
	2453085314864 -> 2453085314960
	2453019619520 [label="depth_encoder.blocks.0.1.bias
 (8)" fillcolor=lightblue]
	2453019619520 -> 2453085314864
	2453085314864 [label=AccumulateGrad]
	2453085314720 -> 2453085314576
	2453019612800 [label="depth_encoder.blocks.0.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	2453019612800 -> 2453085314720
	2453085314720 [label=AccumulateGrad]
	2453085314672 -> 2453085314576
	2453019612720 [label="depth_encoder.blocks.0.3.bias
 (8)" fillcolor=lightblue]
	2453019612720 -> 2453085314672
	2453085314672 [label=AccumulateGrad]
	2453085314528 -> 2453085314480
	2453019619040 [label="depth_encoder.blocks.0.4.weight
 (8)" fillcolor=lightblue]
	2453019619040 -> 2453085314528
	2453085314528 [label=AccumulateGrad]
	2453085314288 -> 2453085314480
	2453019618960 [label="depth_encoder.blocks.0.4.bias
 (8)" fillcolor=lightblue]
	2453019618960 -> 2453085314288
	2453085314288 [label=AccumulateGrad]
	2453085314144 -> 2453085314000
	2453019612240 [label="depth_encoder.blocks.1.0.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	2453019612240 -> 2453085314144
	2453085314144 [label=AccumulateGrad]
	2453085314096 -> 2453085314000
	2453019618720 [label="depth_encoder.blocks.1.0.bias
 (16)" fillcolor=lightblue]
	2453019618720 -> 2453085314096
	2453085314096 [label=AccumulateGrad]
	2453085313952 -> 2453085313904
	2453019618640 [label="depth_encoder.blocks.1.1.weight
 (16)" fillcolor=lightblue]
	2453019618640 -> 2453085313952
	2453085313952 [label=AccumulateGrad]
	2453085313808 -> 2453085313904
	2453019612160 [label="depth_encoder.blocks.1.1.bias
 (16)" fillcolor=lightblue]
	2453019612160 -> 2453085313808
	2453085313808 [label=AccumulateGrad]
	2453085313664 -> 2453085313520
	2453019618400 [label="depth_encoder.blocks.1.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	2453019618400 -> 2453085313664
	2453085313664 [label=AccumulateGrad]
	2453085313616 -> 2453085313520
	2453019618320 [label="depth_encoder.blocks.1.3.bias
 (16)" fillcolor=lightblue]
	2453019618320 -> 2453085313616
	2453085313616 [label=AccumulateGrad]
	2453085313472 -> 2453085313424
	2453019611840 [label="depth_encoder.blocks.1.4.weight
 (16)" fillcolor=lightblue]
	2453019611840 -> 2453085313472
	2453085313472 [label=AccumulateGrad]
	2453085313040 -> 2453085313424
	2453019611760 [label="depth_encoder.blocks.1.4.bias
 (16)" fillcolor=lightblue]
	2453019611760 -> 2453085313040
	2453085313040 [label=AccumulateGrad]
	2453085312656 -> 2453085312080
	2453019619120 [label="depth_encoder.blocks.2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	2453019619120 -> 2453085312656
	2453085312656 [label=AccumulateGrad]
	2453085313184 -> 2453085312080
	2453019612640 [label="depth_encoder.blocks.2.0.bias
 (32)" fillcolor=lightblue]
	2453019612640 -> 2453085313184
	2453085313184 [label=AccumulateGrad]
	2453085311984 -> 2453085312464
	2453019612560 [label="depth_encoder.blocks.2.1.weight
 (32)" fillcolor=lightblue]
	2453019612560 -> 2453085311984
	2453085311984 [label=AccumulateGrad]
	2453085311456 -> 2453085312464
	2453019607920 [label="depth_encoder.blocks.2.1.bias
 (32)" fillcolor=lightblue]
	2453019607920 -> 2453085311456
	2453085311456 [label=AccumulateGrad]
	2453085311264 -> 2453085310832
	2453019623504 [label="depth_encoder.blocks.2.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2453019623504 -> 2453085311264
	2453085311264 [label=AccumulateGrad]
	2453085310784 -> 2453085310832
	2453019630304 [label="depth_encoder.blocks.2.3.bias
 (32)" fillcolor=lightblue]
	2453019630304 -> 2453085310784
	2453085310784 [label=AccumulateGrad]
	2453085310256 -> 2453085310352
	2453019630224 [label="depth_encoder.blocks.2.4.weight
 (32)" fillcolor=lightblue]
	2453019630224 -> 2453085310256
	2453085310256 [label=AccumulateGrad]
	2453085309680 -> 2453085310352
	2453019630144 [label="depth_encoder.blocks.2.4.bias
 (32)" fillcolor=lightblue]
	2453019630144 -> 2453085309680
	2453085309680 [label=AccumulateGrad]
	2453085308624 -> 2453085308720
	2453085308624 [label=TBackward0]
	2453085309152 -> 2453085308624
	2453019629584 [label="attention.mha.in_proj_weight
 (192, 64)" fillcolor=lightblue]
	2453019629584 -> 2453085309152
	2453085309152 [label=AccumulateGrad]
	2453085308432 -> 2453085308336
	2453019629664 [label="attention.mha.in_proj_bias
 (192)" fillcolor=lightblue]
	2453019629664 -> 2453085308432
	2453085308432 [label=AccumulateGrad]
	2453085307424 -> 2453085307376
	2453085307424 [label=TransposeBackward0]
	2453060250464 -> 2453085307424
	2453060250464 [label=TransposeBackward0]
	2453085307856 -> 2453060250464
	2453085307856 [label=ViewBackward0]
	2453085308048 -> 2453085307856
	2453085308048 [label=SelectBackward0]
	2453085307904 -> 2453085308048
	2453085307184 -> 2453085307136
	2453085307184 [label=TransposeBackward0]
	2453085307664 -> 2453085307184
	2453085307664 [label=ViewBackward0]
	2453085307952 -> 2453085307664
	2453085307952 [label=SelectBackward0]
	2453085307904 -> 2453085307952
	2453084929328 -> 2453084929808
	2453084929328 [label=TBackward0]
	2453058824720 -> 2453084929328
	2453019629344 [label="attention.mha.out_proj.weight
 (64, 64)" fillcolor=lightblue]
	2453019629344 -> 2453058824720
	2453058824720 [label=AccumulateGrad]
	2453084929232 -> 2453084929088
	2453084929232 [label=CatBackward0]
	2453085309728 -> 2453084929232
	2453085310064 -> 2453084929232
	2453084929040 -> 2453084928896
	2453019628544 [label="decoder.blocks.0.conv.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	2453019628544 -> 2453084929040
	2453084929040 [label=AccumulateGrad]
	2453084928992 -> 2453084928896
	2453019628464 [label="decoder.blocks.0.conv.0.bias
 (32)" fillcolor=lightblue]
	2453019628464 -> 2453084928992
	2453084928992 [label=AccumulateGrad]
	2453084928848 -> 2453084928800
	2453019628384 [label="decoder.blocks.0.conv.1.weight
 (32)" fillcolor=lightblue]
	2453019628384 -> 2453084928848
	2453084928848 [label=AccumulateGrad]
	2453084928704 -> 2453084928800
	2453019628304 [label="decoder.blocks.0.conv.1.bias
 (32)" fillcolor=lightblue]
	2453019628304 -> 2453084928704
	2453084928704 [label=AccumulateGrad]
	2453084928560 -> 2453084928416
	2453019627824 [label="decoder.blocks.0.conv.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2453019627824 -> 2453084928560
	2453084928560 [label=AccumulateGrad]
	2453084928512 -> 2453084928416
	2453019627744 [label="decoder.blocks.0.conv.3.bias
 (32)" fillcolor=lightblue]
	2453019627744 -> 2453084928512
	2453084928512 [label=AccumulateGrad]
	2453084928368 -> 2453084928320
	2453019627664 [label="decoder.blocks.0.conv.4.weight
 (32)" fillcolor=lightblue]
	2453019627664 -> 2453084928368
	2453084928368 [label=AccumulateGrad]
	2453084928128 -> 2453084928320
	2453019627584 [label="decoder.blocks.0.conv.4.bias
 (32)" fillcolor=lightblue]
	2453019627584 -> 2453084928128
	2453084928128 [label=AccumulateGrad]
	2453084928032 -> 2453084927888
	2453084928032 [label=CatBackward0]
	2453085310928 -> 2453084928032
	2453085313328 -> 2453084928032
	2453084927840 -> 2453084927696
	2453019627104 [label="decoder.blocks.1.conv.0.weight
 (16, 64, 3, 3)" fillcolor=lightblue]
	2453019627104 -> 2453084927840
	2453084927840 [label=AccumulateGrad]
	2453084927792 -> 2453084927696
	2453019627024 [label="decoder.blocks.1.conv.0.bias
 (16)" fillcolor=lightblue]
	2453019627024 -> 2453084927792
	2453084927792 [label=AccumulateGrad]
	2453084927648 -> 2453084927600
	2453019626784 [label="decoder.blocks.1.conv.1.weight
 (16)" fillcolor=lightblue]
	2453019626784 -> 2453084927648
	2453084927648 [label=AccumulateGrad]
	2453084927504 -> 2453084927600
	2453019626704 [label="decoder.blocks.1.conv.1.bias
 (16)" fillcolor=lightblue]
	2453019626704 -> 2453084927504
	2453084927504 [label=AccumulateGrad]
	2453084927360 -> 2453084927216
	2453019626384 [label="decoder.blocks.1.conv.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	2453019626384 -> 2453084927360
	2453084927360 [label=AccumulateGrad]
	2453084927312 -> 2453084927216
	2453019626304 [label="decoder.blocks.1.conv.3.bias
 (16)" fillcolor=lightblue]
	2453019626304 -> 2453084927312
	2453084927312 [label=AccumulateGrad]
	2453084927168 -> 2453084927120
	2453019626224 [label="decoder.blocks.1.conv.4.weight
 (16)" fillcolor=lightblue]
	2453019626224 -> 2453084927168
	2453084927168 [label=AccumulateGrad]
	2453084926976 -> 2453084927120
	2453019626144 [label="decoder.blocks.1.conv.4.bias
 (16)" fillcolor=lightblue]
	2453019626144 -> 2453084926976
	2453084926976 [label=AccumulateGrad]
	2453084926880 -> 2453084926736
	2453084926880 [label=CatBackward0]
	2453085312128 -> 2453084926880
	2453085314384 -> 2453084926880
	2453084926688 -> 2453084926544
	2453019625504 [label="decoder.blocks.2.conv.0.weight
 (8, 32, 3, 3)" fillcolor=lightblue]
	2453019625504 -> 2453084926688
	2453084926688 [label=AccumulateGrad]
	2453084926640 -> 2453084926544
	2453019625424 [label="decoder.blocks.2.conv.0.bias
 (8)" fillcolor=lightblue]
	2453019625424 -> 2453084926640
	2453084926640 [label=AccumulateGrad]
	2453084926496 -> 2453084926448
	2453019625344 [label="decoder.blocks.2.conv.1.weight
 (8)" fillcolor=lightblue]
	2453019625344 -> 2453084926496
	2453084926496 [label=AccumulateGrad]
	2453084926352 -> 2453084926448
	2453019625264 [label="decoder.blocks.2.conv.1.bias
 (8)" fillcolor=lightblue]
	2453019625264 -> 2453084926352
	2453084926352 [label=AccumulateGrad]
	2453084926112 -> 2453084926160
	2453019625904 [label="decoder.blocks.2.conv.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	2453019625904 -> 2453084926112
	2453084926112 [label=AccumulateGrad]
	2453084926208 -> 2453084926160
	2453019623744 [label="decoder.blocks.2.conv.3.bias
 (8)" fillcolor=lightblue]
	2453019623744 -> 2453084926208
	2453084926208 [label=AccumulateGrad]
	2453084925776 -> 2453067098352
	2453019623664 [label="decoder.blocks.2.conv.4.weight
 (8)" fillcolor=lightblue]
	2453019623664 -> 2453084925776
	2453084925776 [label=AccumulateGrad]
	2453084925920 -> 2453067098352
	2453019630464 [label="decoder.blocks.2.conv.4.bias
 (8)" fillcolor=lightblue]
	2453019630464 -> 2453084925920
	2453084925920 [label=AccumulateGrad]
	2453084926064 -> 2452868070128
	2453085068880 [label="decoder.final_conv.weight
 (3, 8, 1, 1)" fillcolor=lightblue]
	2453085068880 -> 2453084926064
	2453084926064 [label=AccumulateGrad]
	2453084926016 -> 2452868070128
	2453085068960 [label="decoder.final_conv.bias
 (3)" fillcolor=lightblue]
	2453085068960 -> 2453084926016
	2453084926016 [label=AccumulateGrad]
	2452868070128 -> 2453085074320
}
