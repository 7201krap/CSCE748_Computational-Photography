digraph {
	graph [size="44.699999999999996,44.699999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1209538398320 [label="
 (1, 3, 240, 320)" fillcolor=darkolivegreen1]
	1209517657968 [label=SigmoidBackward0]
	1209518379584 -> 1209517657968
	1209518379584 [label=ConvolutionBackward0]
	1209518615152 -> 1209518379584
	1209518615152 [label=ReluBackward0]
	1209519775872 -> 1209518615152
	1209519775872 [label=NativeBatchNormBackward0]
	1209519874608 -> 1209519775872
	1209519874608 [label=ConvolutionBackward0]
	1209536926320 -> 1209519874608
	1209536926320 [label=ReluBackward0]
	1209501649984 -> 1209536926320
	1209501649984 [label=NativeBatchNormBackward0]
	1209537348128 -> 1209501649984
	1209537348128 [label=ConvolutionBackward0]
	1209536801056 -> 1209537348128
	1209536801056 [label=CatBackward0]
	1209536654512 -> 1209536801056
	1209536654512 [label=UpsampleBilinear2DBackward0]
	1209537783616 -> 1209536654512
	1209537783616 [label=ReluBackward0]
	1209537783136 -> 1209537783616
	1209537783136 [label=NativeBatchNormBackward0]
	1209537783040 -> 1209537783136
	1209537783040 [label=ConvolutionBackward0]
	1209537781888 -> 1209537783040
	1209537781888 [label=ReluBackward0]
	1209537782848 -> 1209537781888
	1209537782848 [label=NativeBatchNormBackward0]
	1209537782800 -> 1209537782848
	1209537782800 [label=ConvolutionBackward0]
	1209537782704 -> 1209537782800
	1209537782704 [label=CatBackward0]
	1209537782608 -> 1209537782704
	1209537782608 [label=UpsampleBilinear2DBackward0]
	1209537781792 -> 1209537782608
	1209537781792 [label=ReluBackward0]
	1209537782128 -> 1209537781792
	1209537782128 [label=NativeBatchNormBackward0]
	1209537782080 -> 1209537782128
	1209537782080 [label=ConvolutionBackward0]
	1209537781984 -> 1209537782080
	1209537781984 [label=ReluBackward0]
	1209537781168 -> 1209537781984
	1209537781168 [label=NativeBatchNormBackward0]
	1209537787216 -> 1209537781168
	1209537787216 [label=ConvolutionBackward0]
	1209538026208 -> 1209537787216
	1209538026208 [label=CatBackward0]
	1209538026304 -> 1209538026208
	1209538026304 [label=UpsampleBilinear2DBackward0]
	1209538026496 -> 1209538026304
	1209538026496 [label=MaxPool2DWithIndicesBackward0]
	1209538026256 -> 1209538026496
	1209538026256 [label=ReluBackward0]
	1209538026736 -> 1209538026256
	1209538026736 [label=NativeBatchNormBackward0]
	1209538026688 -> 1209538026736
	1209538026688 [label=ConvolutionBackward0]
	1209538026880 -> 1209538026688
	1209538026880 [label=ReluBackward0]
	1209538019536 -> 1209538026880
	1209538019536 [label=NativeBatchNormBackward0]
	1209538019584 -> 1209538019536
	1209538019584 [label=ConvolutionBackward0]
	1209538019872 -> 1209538019584
	1209538019872 [label=MaxPool2DWithIndicesBackward0]
	1209537782224 -> 1209538019872
	1209537782224 [label=ReluBackward0]
	1209538019968 -> 1209537782224
	1209538019968 [label=NativeBatchNormBackward0]
	1209538020208 -> 1209538019968
	1209538020208 [label=ConvolutionBackward0]
	1209538020496 -> 1209538020208
	1209538020496 [label=ReluBackward0]
	1209538020688 -> 1209538020496
	1209538020688 [label=NativeBatchNormBackward0]
	1209538020736 -> 1209538020688
	1209538020736 [label=ConvolutionBackward0]
	1209538021024 -> 1209538020736
	1209538021024 [label=MaxPool2DWithIndicesBackward0]
	1209537783328 -> 1209538021024
	1209537783328 [label=ReluBackward0]
	1209538021168 -> 1209537783328
	1209538021168 [label=NativeBatchNormBackward0]
	1209538021360 -> 1209538021168
	1209538021360 [label=ConvolutionBackward0]
	1209538021648 -> 1209538021360
	1209538021648 [label=ReluBackward0]
	1209538021840 -> 1209538021648
	1209538021840 [label=NativeBatchNormBackward0]
	1209538021888 -> 1209538021840
	1209538021888 [label=ConvolutionBackward0]
	1209538022176 -> 1209538021888
	1209518304912 [label="encoder.blocks.0.0.weight
 (8, 3, 3, 3)" fillcolor=lightblue]
	1209518304912 -> 1209538022176
	1209538022176 [label=AccumulateGrad]
	1209538022128 -> 1209538021888
	1209518309152 [label="encoder.blocks.0.0.bias
 (8)" fillcolor=lightblue]
	1209518309152 -> 1209538022128
	1209538022128 [label=AccumulateGrad]
	1209538021744 -> 1209538021840
	1209518309232 [label="encoder.blocks.0.1.weight
 (8)" fillcolor=lightblue]
	1209518309232 -> 1209538021744
	1209538021744 [label=AccumulateGrad]
	1209538021984 -> 1209538021840
	1209518304992 [label="encoder.blocks.0.1.bias
 (8)" fillcolor=lightblue]
	1209518304992 -> 1209538021984
	1209538021984 [label=AccumulateGrad]
	1209538021600 -> 1209538021360
	1209537881232 [label="encoder.blocks.0.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	1209537881232 -> 1209538021600
	1209538021600 [label=AccumulateGrad]
	1209538021552 -> 1209538021360
	1209537881632 [label="encoder.blocks.0.3.bias
 (8)" fillcolor=lightblue]
	1209537881632 -> 1209538021552
	1209538021552 [label=AccumulateGrad]
	1209538021312 -> 1209538021168
	1209537879712 [label="encoder.blocks.0.4.weight
 (8)" fillcolor=lightblue]
	1209537879712 -> 1209538021312
	1209538021312 [label=AccumulateGrad]
	1209538021456 -> 1209538021168
	1209538057536 [label="encoder.blocks.0.4.bias
 (8)" fillcolor=lightblue]
	1209538057536 -> 1209538021456
	1209538021456 [label=AccumulateGrad]
	1209538020976 -> 1209538020736
	1209538060176 [label="encoder.blocks.1.0.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	1209538060176 -> 1209538020976
	1209538020976 [label=AccumulateGrad]
	1209538020928 -> 1209538020736
	1209538059776 [label="encoder.blocks.1.0.bias
 (16)" fillcolor=lightblue]
	1209538059776 -> 1209538020928
	1209538020928 [label=AccumulateGrad]
	1209538020592 -> 1209538020688
	1209538059856 [label="encoder.blocks.1.1.weight
 (16)" fillcolor=lightblue]
	1209538059856 -> 1209538020592
	1209538020592 [label=AccumulateGrad]
	1209538020832 -> 1209538020688
	1209538060496 [label="encoder.blocks.1.1.bias
 (16)" fillcolor=lightblue]
	1209538060496 -> 1209538020832
	1209538020832 [label=AccumulateGrad]
	1209538020448 -> 1209538020208
	1209538060896 [label="encoder.blocks.1.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1209538060896 -> 1209538020448
	1209538020448 [label=AccumulateGrad]
	1209538020400 -> 1209538020208
	1209538061216 [label="encoder.blocks.1.3.bias
 (16)" fillcolor=lightblue]
	1209538061216 -> 1209538020400
	1209538020400 [label=AccumulateGrad]
	1209538020160 -> 1209538019968
	1209538061296 [label="encoder.blocks.1.4.weight
 (16)" fillcolor=lightblue]
	1209538061296 -> 1209538020160
	1209538020160 [label=AccumulateGrad]
	1209538020304 -> 1209538019968
	1209538061376 [label="encoder.blocks.1.4.bias
 (16)" fillcolor=lightblue]
	1209538061376 -> 1209538020304
	1209538020304 [label=AccumulateGrad]
	1209538019824 -> 1209538019584
	1209538061856 [label="encoder.blocks.2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	1209538061856 -> 1209538019824
	1209538019824 [label=AccumulateGrad]
	1209538019776 -> 1209538019584
	1209538061936 [label="encoder.blocks.2.0.bias
 (32)" fillcolor=lightblue]
	1209538061936 -> 1209538019776
	1209538019776 [label=AccumulateGrad]
	1209538019440 -> 1209538019536
	1209538062016 [label="encoder.blocks.2.1.weight
 (32)" fillcolor=lightblue]
	1209538062016 -> 1209538019440
	1209538019440 [label=AccumulateGrad]
	1209538019680 -> 1209538019536
	1209538062096 [label="encoder.blocks.2.1.bias
 (32)" fillcolor=lightblue]
	1209538062096 -> 1209538019680
	1209538019680 [label=AccumulateGrad]
	1209538026976 -> 1209538026688
	1209538062576 [label="encoder.blocks.2.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	1209538062576 -> 1209538026976
	1209538026976 [label=AccumulateGrad]
	1209538026928 -> 1209538026688
	1209538062656 [label="encoder.blocks.2.3.bias
 (32)" fillcolor=lightblue]
	1209538062656 -> 1209538026928
	1209538026928 [label=AccumulateGrad]
	1209538026400 -> 1209538026736
	1209538065376 [label="encoder.blocks.2.4.weight
 (32)" fillcolor=lightblue]
	1209538065376 -> 1209538026400
	1209538026400 [label=AccumulateGrad]
	1209538026832 -> 1209538026736
	1209538065456 [label="encoder.blocks.2.4.bias
 (32)" fillcolor=lightblue]
	1209538065456 -> 1209538026832
	1209538026832 [label=AccumulateGrad]
	1209538026256 -> 1209538026208
	1209538027120 -> 1209537787216
	1209538065936 [label="decoder.blocks.0.conv.0.weight
 (16, 64, 3, 3)" fillcolor=lightblue]
	1209538065936 -> 1209538027120
	1209538027120 [label=AccumulateGrad]
	1209538027216 -> 1209537787216
	1209538066016 [label="decoder.blocks.0.conv.0.bias
 (16)" fillcolor=lightblue]
	1209538066016 -> 1209538027216
	1209538027216 [label=AccumulateGrad]
	1209537781072 -> 1209537781168
	1209538066096 [label="decoder.blocks.0.conv.1.weight
 (16)" fillcolor=lightblue]
	1209538066096 -> 1209537781072
	1209537781072 [label=AccumulateGrad]
	1209537781408 -> 1209537781168
	1209538066176 [label="decoder.blocks.0.conv.1.bias
 (16)" fillcolor=lightblue]
	1209538066176 -> 1209537781408
	1209537781408 [label=AccumulateGrad]
	1209537781600 -> 1209537782080
	1209538066496 [label="decoder.blocks.0.conv.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	1209538066496 -> 1209537781600
	1209537781600 [label=AccumulateGrad]
	1209537782032 -> 1209537782080
	1209538066656 [label="decoder.blocks.0.conv.3.bias
 (16)" fillcolor=lightblue]
	1209538066656 -> 1209537782032
	1209537782032 [label=AccumulateGrad]
	1209537781744 -> 1209537782128
	1209538066736 [label="decoder.blocks.0.conv.4.weight
 (16)" fillcolor=lightblue]
	1209538066736 -> 1209537781744
	1209537781744 [label=AccumulateGrad]
	1209537781552 -> 1209537782128
	1209538066816 [label="decoder.blocks.0.conv.4.bias
 (16)" fillcolor=lightblue]
	1209538066816 -> 1209537781552
	1209537781552 [label=AccumulateGrad]
	1209537782224 -> 1209537782704
	1209537782320 -> 1209537782800
	1209538067296 [label="decoder.blocks.1.conv.0.weight
 (8, 32, 3, 3)" fillcolor=lightblue]
	1209538067296 -> 1209537782320
	1209537782320 [label=AccumulateGrad]
	1209537782752 -> 1209537782800
	1209538067456 [label="decoder.blocks.1.conv.0.bias
 (8)" fillcolor=lightblue]
	1209538067456 -> 1209537782752
	1209537782752 [label=AccumulateGrad]
	1209537782416 -> 1209537782848
	1209538067376 [label="decoder.blocks.1.conv.1.weight
 (8)" fillcolor=lightblue]
	1209538067376 -> 1209537782416
	1209537782416 [label=AccumulateGrad]
	1209537782512 -> 1209537782848
	1209538067536 [label="decoder.blocks.1.conv.1.bias
 (8)" fillcolor=lightblue]
	1209538067536 -> 1209537782512
	1209537782512 [label=AccumulateGrad]
	1209537783376 -> 1209537783040
	1209538068016 [label="decoder.blocks.1.conv.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	1209538068016 -> 1209537783376
	1209537783376 [label=AccumulateGrad]
	1209537782992 -> 1209537783040
	1209538068096 [label="decoder.blocks.1.conv.3.bias
 (8)" fillcolor=lightblue]
	1209538068096 -> 1209537782992
	1209537782992 [label=AccumulateGrad]
	1209537783088 -> 1209537783136
	1209538068176 [label="decoder.blocks.1.conv.4.weight
 (8)" fillcolor=lightblue]
	1209538068176 -> 1209537783088
	1209537783088 [label=AccumulateGrad]
	1209537783280 -> 1209537783136
	1209538068256 [label="decoder.blocks.1.conv.4.bias
 (8)" fillcolor=lightblue]
	1209538068256 -> 1209537783280
	1209537783280 [label=AccumulateGrad]
	1209537783328 -> 1209536801056
	1209536643568 -> 1209537348128
	1209538396400 [label="decoder.blocks.2.conv.0.weight
 (4, 16, 3, 3)" fillcolor=lightblue]
	1209538396400 -> 1209536643568
	1209536643568 [label=AccumulateGrad]
	1209537783712 -> 1209537348128
	1209538396480 [label="decoder.blocks.2.conv.0.bias
 (4)" fillcolor=lightblue]
	1209538396480 -> 1209537783712
	1209537783712 [label=AccumulateGrad]
	1209537599552 -> 1209501649984
	1209538396560 [label="decoder.blocks.2.conv.1.weight
 (4)" fillcolor=lightblue]
	1209538396560 -> 1209537599552
	1209537599552 [label=AccumulateGrad]
	1209536643616 -> 1209501649984
	1209538396640 [label="decoder.blocks.2.conv.1.bias
 (4)" fillcolor=lightblue]
	1209538396640 -> 1209536643616
	1209536643616 [label=AccumulateGrad]
	1209517068768 -> 1209519874608
	1209538397120 [label="decoder.blocks.2.conv.3.weight
 (4, 4, 3, 3)" fillcolor=lightblue]
	1209538397120 -> 1209517068768
	1209517068768 [label=AccumulateGrad]
	1209518290112 -> 1209519874608
	1209538397200 [label="decoder.blocks.2.conv.3.bias
 (4)" fillcolor=lightblue]
	1209538397200 -> 1209518290112
	1209518290112 [label=AccumulateGrad]
	1210686147888 -> 1209519775872
	1209538397280 [label="decoder.blocks.2.conv.4.weight
 (4)" fillcolor=lightblue]
	1209538397280 -> 1210686147888
	1210686147888 [label=AccumulateGrad]
	1209518287232 -> 1209519775872
	1209538397360 [label="decoder.blocks.2.conv.4.bias
 (4)" fillcolor=lightblue]
	1209538397360 -> 1209518287232
	1209518287232 [label=AccumulateGrad]
	1209518620048 -> 1209518379584
	1209538397760 [label="decoder.final_conv.weight
 (3, 4, 1, 1)" fillcolor=lightblue]
	1209538397760 -> 1209518620048
	1209518620048 [label=AccumulateGrad]
	1209519156304 -> 1209518379584
	1209538397840 [label="decoder.final_conv.bias
 (3)" fillcolor=lightblue]
	1209538397840 -> 1209519156304
	1209519156304 [label=AccumulateGrad]
	1209517657968 -> 1209538398320
}
