digraph {
	graph [size="44.699999999999996,44.699999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2646291581760 [label="
 (1, 3, 240, 320)" fillcolor=darkolivegreen1]
	2646339934560 [label=SigmoidBackward0]
	2646356847120 -> 2646339934560
	2646356847120 [label=ConvolutionBackward0]
	2646356846976 -> 2646356847120
	2646356846976 [label=ReluBackward0]
	2646142433888 -> 2646356846976
	2646142433888 [label=NativeBatchNormBackward0]
	2646356847456 -> 2646142433888
	2646356847456 [label=ConvolutionBackward0]
	2646356847648 -> 2646356847456
	2646356847648 [label=ReluBackward0]
	2646356847840 -> 2646356847648
	2646356847840 [label=NativeBatchNormBackward0]
	2646356847936 -> 2646356847840
	2646356847936 [label=ConvolutionBackward0]
	2646356848128 -> 2646356847936
	2646356848128 [label=CatBackward0]
	2646356848320 -> 2646356848128
	2646356848320 [label=UpsampleBilinear2DBackward0]
	2646356848464 -> 2646356848320
	2646356848464 [label=ReluBackward0]
	2646356848560 -> 2646356848464
	2646356848560 [label=NativeBatchNormBackward0]
	2646356848656 -> 2646356848560
	2646356848656 [label=ConvolutionBackward0]
	2646356848848 -> 2646356848656
	2646356848848 [label=ReluBackward0]
	2646356849040 -> 2646356848848
	2646356849040 [label=NativeBatchNormBackward0]
	2646356849136 -> 2646356849040
	2646356849136 [label=ConvolutionBackward0]
	2646356849328 -> 2646356849136
	2646356849328 [label=CatBackward0]
	2646356849520 -> 2646356849328
	2646356849520 [label=UpsampleBilinear2DBackward0]
	2646356849664 -> 2646356849520
	2646356849664 [label=ReluBackward0]
	2646356849760 -> 2646356849664
	2646356849760 [label=NativeBatchNormBackward0]
	2646356849856 -> 2646356849760
	2646356849856 [label=ConvolutionBackward0]
	2646356850048 -> 2646356849856
	2646356850048 [label=ReluBackward0]
	2646356850240 -> 2646356850048
	2646356850240 [label=NativeBatchNormBackward0]
	2646356850336 -> 2646356850240
	2646356850336 [label=ConvolutionBackward0]
	2646356850528 -> 2646356850336
	2646356850528 [label=CatBackward0]
	2646356850720 -> 2646356850528
	2646356850720 [label=UpsampleBilinear2DBackward0]
	2646356850864 -> 2646356850720
	2646356850864 [label=MaxPool2DWithIndicesBackward0]
	2646356850672 -> 2646356850864
	2646356850672 [label=ReluBackward0]
	2646356851008 -> 2646356850672
	2646356851008 [label=NativeBatchNormBackward0]
	2646356851104 -> 2646356851008
	2646356851104 [label=ConvolutionBackward0]
	2646356851296 -> 2646356851104
	2646356851296 [label=ReluBackward0]
	2646356851488 -> 2646356851296
	2646356851488 [label=NativeBatchNormBackward0]
	2646356851584 -> 2646356851488
	2646356851584 [label=ConvolutionBackward0]
	2646356851776 -> 2646356851584
	2646356851776 [label=MaxPool2DWithIndicesBackward0]
	2646356849472 -> 2646356851776
	2646356849472 [label=ReluBackward0]
	2646356852016 -> 2646356849472
	2646356852016 [label=NativeBatchNormBackward0]
	2646356852112 -> 2646356852016
	2646356852112 [label=ConvolutionBackward0]
	2646356852304 -> 2646356852112
	2646356852304 [label=ReluBackward0]
	2646356852496 -> 2646356852304
	2646356852496 [label=NativeBatchNormBackward0]
	2646356852592 -> 2646356852496
	2646356852592 [label=ConvolutionBackward0]
	2646356852688 -> 2646356852592
	2646356852688 [label=MaxPool2DWithIndicesBackward0]
	2646356848272 -> 2646356852688
	2646356848272 [label=ReluBackward0]
	2646357082464 -> 2646356848272
	2646357082464 [label=NativeBatchNormBackward0]
	2646357082560 -> 2646357082464
	2646357082560 [label=ConvolutionBackward0]
	2646357082752 -> 2646357082560
	2646357082752 [label=ReluBackward0]
	2646357082944 -> 2646357082752
	2646357082944 [label=NativeBatchNormBackward0]
	2646357083040 -> 2646357082944
	2646357083040 [label=ConvolutionBackward0]
	2646357083232 -> 2646357083040
	2646291562032 [label="encoder.blocks.0.0.weight
 (8, 3, 3, 3)" fillcolor=lightblue]
	2646291562032 -> 2646357083232
	2646357083232 [label=AccumulateGrad]
	2646357083184 -> 2646357083040
	2646291555312 [label="encoder.blocks.0.0.bias
 (8)" fillcolor=lightblue]
	2646291555312 -> 2646357083184
	2646357083184 [label=AccumulateGrad]
	2646357082992 -> 2646357082944
	2646291554912 [label="encoder.blocks.0.1.weight
 (8)" fillcolor=lightblue]
	2646291554912 -> 2646357082992
	2646357082992 [label=AccumulateGrad]
	2646357082848 -> 2646357082944
	2646291561552 [label="encoder.blocks.0.1.bias
 (8)" fillcolor=lightblue]
	2646291561552 -> 2646357082848
	2646357082848 [label=AccumulateGrad]
	2646357082704 -> 2646357082560
	2646291554192 [label="encoder.blocks.0.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	2646291554192 -> 2646357082704
	2646357082704 [label=AccumulateGrad]
	2646357082656 -> 2646357082560
	2646291554112 [label="encoder.blocks.0.3.bias
 (8)" fillcolor=lightblue]
	2646291554112 -> 2646357082656
	2646357082656 [label=AccumulateGrad]
	2646357082512 -> 2646357082464
	2646291560752 [label="encoder.blocks.0.4.weight
 (8)" fillcolor=lightblue]
	2646291560752 -> 2646357082512
	2646357082512 [label=AccumulateGrad]
	2646357082368 -> 2646357082464
	2646291560672 [label="encoder.blocks.0.4.bias
 (8)" fillcolor=lightblue]
	2646291560672 -> 2646357082368
	2646357082368 [label=AccumulateGrad]
	2646357082224 -> 2646356852592
	2646291553632 [label="encoder.blocks.1.0.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	2646291553632 -> 2646357082224
	2646357082224 [label=AccumulateGrad]
	2646357082176 -> 2646356852592
	2646291560272 [label="encoder.blocks.1.0.bias
 (16)" fillcolor=lightblue]
	2646291560272 -> 2646357082176
	2646357082176 [label=AccumulateGrad]
	2646356852544 -> 2646356852496
	2646291560192 [label="encoder.blocks.1.1.weight
 (16)" fillcolor=lightblue]
	2646291560192 -> 2646356852544
	2646356852544 [label=AccumulateGrad]
	2646356852400 -> 2646356852496
	2646291553552 [label="encoder.blocks.1.1.bias
 (16)" fillcolor=lightblue]
	2646291553552 -> 2646356852400
	2646356852400 [label=AccumulateGrad]
	2646356852208 -> 2646356852112
	2646291559952 [label="encoder.blocks.1.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	2646291559952 -> 2646356852208
	2646356852208 [label=AccumulateGrad]
	2646356852256 -> 2646356852112
	2646291559872 [label="encoder.blocks.1.3.bias
 (16)" fillcolor=lightblue]
	2646291559872 -> 2646356852256
	2646356852256 [label=AccumulateGrad]
	2646356852064 -> 2646356852016
	2646291553232 [label="encoder.blocks.1.4.weight
 (16)" fillcolor=lightblue]
	2646291553232 -> 2646356852064
	2646356852064 [label=AccumulateGrad]
	2646356851920 -> 2646356852016
	2646291553152 [label="encoder.blocks.1.4.bias
 (16)" fillcolor=lightblue]
	2646291553152 -> 2646356851920
	2646356851920 [label=AccumulateGrad]
	2646356851728 -> 2646356851584
	2646291559552 [label="encoder.blocks.2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	2646291559552 -> 2646356851728
	2646356851728 [label=AccumulateGrad]
	2646356851680 -> 2646356851584
	2646291552912 [label="encoder.blocks.2.0.bias
 (32)" fillcolor=lightblue]
	2646291552912 -> 2646356851680
	2646356851680 [label=AccumulateGrad]
	2646356851536 -> 2646356851488
	2646291552832 [label="encoder.blocks.2.1.weight
 (32)" fillcolor=lightblue]
	2646291552832 -> 2646356851536
	2646356851536 [label=AccumulateGrad]
	2646356851392 -> 2646356851488
	2646291559472 [label="encoder.blocks.2.1.bias
 (32)" fillcolor=lightblue]
	2646291559472 -> 2646356851392
	2646356851392 [label=AccumulateGrad]
	2646356851248 -> 2646356851104
	2646291552752 [label="encoder.blocks.2.3.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2646291552752 -> 2646356851248
	2646356851248 [label=AccumulateGrad]
	2646356851200 -> 2646356851104
	2646291552672 [label="encoder.blocks.2.3.bias
 (32)" fillcolor=lightblue]
	2646291552672 -> 2646356851200
	2646356851200 [label=AccumulateGrad]
	2646356851056 -> 2646356851008
	2646291559312 [label="encoder.blocks.2.4.weight
 (32)" fillcolor=lightblue]
	2646291559312 -> 2646356851056
	2646356851056 [label=AccumulateGrad]
	2646356850912 -> 2646356851008
	2646291559232 [label="encoder.blocks.2.4.bias
 (32)" fillcolor=lightblue]
	2646291559232 -> 2646356850912
	2646356850912 [label=AccumulateGrad]
	2646356850672 -> 2646356850528
	2646356850480 -> 2646356850336
	2646291552032 [label="decoder.blocks.0.conv.0.weight
 (16, 64, 3, 3)" fillcolor=lightblue]
	2646291552032 -> 2646356850480
	2646356850480 [label=AccumulateGrad]
	2646356850432 -> 2646356850336
	2646291558672 [label="decoder.blocks.0.conv.0.bias
 (16)" fillcolor=lightblue]
	2646291558672 -> 2646356850432
	2646356850432 [label=AccumulateGrad]
	2646356850288 -> 2646356850240
	2646291558592 [label="decoder.blocks.0.conv.1.weight
 (16)" fillcolor=lightblue]
	2646291558592 -> 2646356850288
	2646356850288 [label=AccumulateGrad]
	2646356850144 -> 2646356850240
	2646291551952 [label="decoder.blocks.0.conv.1.bias
 (16)" fillcolor=lightblue]
	2646291551952 -> 2646356850144
	2646356850144 [label=AccumulateGrad]
	2646356850000 -> 2646356849856
	2646291558192 [label="decoder.blocks.0.conv.3.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	2646291558192 -> 2646356850000
	2646356850000 [label=AccumulateGrad]
	2646356849952 -> 2646356849856
	2646291558112 [label="decoder.blocks.0.conv.3.bias
 (16)" fillcolor=lightblue]
	2646291558112 -> 2646356849952
	2646356849952 [label=AccumulateGrad]
	2646356849808 -> 2646356849760
	2646291551472 [label="decoder.blocks.0.conv.4.weight
 (16)" fillcolor=lightblue]
	2646291551472 -> 2646356849808
	2646356849808 [label=AccumulateGrad]
	2646356849568 -> 2646356849760
	2646291551392 [label="decoder.blocks.0.conv.4.bias
 (16)" fillcolor=lightblue]
	2646291551392 -> 2646356849568
	2646356849568 [label=AccumulateGrad]
	2646356849472 -> 2646356849328
	2646356849280 -> 2646356849136
	2646291557792 [label="decoder.blocks.1.conv.0.weight
 (8, 32, 3, 3)" fillcolor=lightblue]
	2646291557792 -> 2646356849280
	2646356849280 [label=AccumulateGrad]
	2646356849232 -> 2646356849136
	2646291551152 [label="decoder.blocks.1.conv.0.bias
 (8)" fillcolor=lightblue]
	2646291551152 -> 2646356849232
	2646356849232 [label=AccumulateGrad]
	2646356849088 -> 2646356849040
	2646291551072 [label="decoder.blocks.1.conv.1.weight
 (8)" fillcolor=lightblue]
	2646291551072 -> 2646356849088
	2646356849088 [label=AccumulateGrad]
	2646356848944 -> 2646356849040
	2646291557712 [label="decoder.blocks.1.conv.1.bias
 (8)" fillcolor=lightblue]
	2646291557712 -> 2646356848944
	2646356848944 [label=AccumulateGrad]
	2646356848800 -> 2646356848656
	2646291550832 [label="decoder.blocks.1.conv.3.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	2646291550832 -> 2646356848800
	2646356848800 [label=AccumulateGrad]
	2646356848752 -> 2646356848656
	2646291550752 [label="decoder.blocks.1.conv.3.bias
 (8)" fillcolor=lightblue]
	2646291550752 -> 2646356848752
	2646356848752 [label=AccumulateGrad]
	2646356848608 -> 2646356848560
	2646291557392 [label="decoder.blocks.1.conv.4.weight
 (8)" fillcolor=lightblue]
	2646291557392 -> 2646356848608
	2646356848608 [label=AccumulateGrad]
	2646356848368 -> 2646356848560
	2646291557312 [label="decoder.blocks.1.conv.4.bias
 (8)" fillcolor=lightblue]
	2646291557312 -> 2646356848368
	2646356848368 [label=AccumulateGrad]
	2646356848272 -> 2646356848128
	2646356848080 -> 2646356847936
	2646291547152 [label="decoder.blocks.2.conv.0.weight
 (4, 16, 3, 3)" fillcolor=lightblue]
	2646291547152 -> 2646356848080
	2646356848080 [label=AccumulateGrad]
	2646356848032 -> 2646356847936
	2646291547072 [label="decoder.blocks.2.conv.0.bias
 (4)" fillcolor=lightblue]
	2646291547072 -> 2646356848032
	2646356848032 [label=AccumulateGrad]
	2646356847888 -> 2646356847840
	2646291556352 [label="decoder.blocks.2.conv.1.weight
 (4)" fillcolor=lightblue]
	2646291556352 -> 2646356847888
	2646356847888 [label=AccumulateGrad]
	2646356847744 -> 2646356847840
	2646291556272 [label="decoder.blocks.2.conv.1.bias
 (4)" fillcolor=lightblue]
	2646291556272 -> 2646356847744
	2646356847744 [label=AccumulateGrad]
	2646356847600 -> 2646356847456
	2646291585600 [label="decoder.blocks.2.conv.3.weight
 (4, 4, 3, 3)" fillcolor=lightblue]
	2646291585600 -> 2646356847600
	2646356847600 [label=AccumulateGrad]
	2646356847552 -> 2646356847456
	2646291585520 [label="decoder.blocks.2.conv.3.bias
 (4)" fillcolor=lightblue]
	2646291585520 -> 2646356847552
	2646356847552 [label=AccumulateGrad]
	2646356847408 -> 2646142433888
	2646291585440 [label="decoder.blocks.2.conv.4.weight
 (4)" fillcolor=lightblue]
	2646291585440 -> 2646356847408
	2646356847408 [label=AccumulateGrad]
	2646356847360 -> 2646142433888
	2646291585360 [label="decoder.blocks.2.conv.4.bias
 (4)" fillcolor=lightblue]
	2646291585360 -> 2646356847360
	2646356847360 [label=AccumulateGrad]
	2646356847024 -> 2646356847120
	2646291584880 [label="decoder.final_conv.weight
 (3, 4, 1, 1)" fillcolor=lightblue]
	2646291584880 -> 2646356847024
	2646356847024 [label=AccumulateGrad]
	2646356847168 -> 2646356847120
	2646291584800 [label="decoder.final_conv.bias
 (3)" fillcolor=lightblue]
	2646291584800 -> 2646356847168
	2646356847168 [label=AccumulateGrad]
	2646339934560 -> 2646291581760
}
